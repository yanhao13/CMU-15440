# CMU-15440
http://www.composablesystems.org/15-440/fa2024/<br />
### Introduction
when does a decentralized system become distributed? in general, adding k>0 links bt two nodes in a decentralized system, alternative approach exists when 2 views on realizing distributed systems: integrative view -connecting existing networked computer systems into a larger system, expansive view -an existing networked computer systems is extended with additional computers, hence, 2 definitions: a decentralized system is a networked computer system in which processes and resources are necessarily spread across multiple computers, a distributed system is a networked computer system in which processes and resources are sufficiently spread across multiple computers;<br /><br />
some common misconceptions including: centralized solutions do not scale -make distinction bt logically and physically centralized, the root of dns including: logically centralized, physically(massively) distributed, decentralized across several organizations; centralized solutions have a single point of failure -generally not true such as the root of dns, a single point of failure is often easier to manage, and easier to make more robust; note: there are many, poorly founded, misconceptions regarding scalability, fault tolerance, security, etc., we need to develop skills by which distributed systems can be readily understood so as to judge such misconceptions;<br /><br />
distributed systems are complex taking perspectives of: architecture -common organizations, process -what kind of processes and their relationships, comm -facilities for exchanging data, coordination -application-independent algorithms, naming -how do you identify resources?, consistency and replication -performance requires of data which need to be the same, fault tolerance -keep running in the presence of partial failures, security -ensure authorized access to resources;<br /><br />
what do we want to achieve? overall design goals including: support sharing of resources, distribution transparency, openness, scalability;<br /><br />
sharing resources: canonnical examples including: cloud-based shared storage and files, p2p assisted multimedia streaming, shared mail services such as outsourced mail systems, shared web hosting such as content distribution networks; observation -“the network is the computer,” quote from John Gage, then at Sun Microsystems;<br /><br />
distribution transparency: transparency -the phenomenon by which a distributed system attempts to hide the fact that its processes and resources are physically distributed across multiple computers, possibly separated by large distances; observation -distribution transparency is handled through many different techniques in a layer bt applications and os, i.e., a mw layer; types including: access -hide diffs in data representation and how an object is accessed, location -hide where an object is located, relocation -hide that an object may be moved to another location while in use, migration -hide that an object may move to another location, replication -hide that an object is replicated, concurrency -hide that an object may be shared by several independent users, failure -hide the failure and recovery of an object;<br /><br />
degree of transparency including: (i) aiming at full distribution transparency may be too much: there are comm latencies that cannot be hidden; completely hiding failures of networks and nodes is(theoretically and practically) impossible, i.e., you cannot distinguish a slow computer from a failing one, and you cannot be sure that a server actually performed an op before a crash; full transparency will cost perf, exposing distribution of the system, i.e., keeping replicas exactly up-to-date with the master takes time, and immediately flushing write op to disk for fault tolerance;<br /><br />
(ii) exposing distribution may be too good: making use of location-based services -finding your nearby friends, when dealing with users in different time zones, when it makes it easier for a user to understand what's going on -when for example, a server does not respond for a long time, report it as failing; conclusion -distribution transparency is a nice goal, but achieving it is a different story, and it should often not even be aimed at;<br /><br />
openness of distributed systems: open -a system that offers components that can easily be used by, or integrated into other systems, an open distributed system itself will often consist of components that originate from elsewhere; what are we talking about? be able to interact with services from other open systems, irrespective of the underlying environment -systems should be conform to well-defined interfaces, systems should be easily interoperate, systems should support portability of applications, systems should be easily extensible;<br /><br />
policies vs. mechanisms: policies including: what level of consistency do we require for client-based data? which op do we allow downloaded code to perform? which qos requirements do we adjust in the face of varying bandwidth? what level of secrecy do we require for comm? mechanisms including: allow (dynamic) setting of caching policies, support different levels of trust for mobile code, provide adjustable qos params per data stream, offer different encryption algorithms;<br /><br />
on strict separation: observation -the stricter the separation bt policy and mechanism, the more we need to ensure proper mechanisms, potentially leading to many config params and complex mgmt; finding a balance -hard-coding policies often simplifies mgmt, and reduces complexity at the price of less flexibility, there is no obvious solution;<br /><br />
一些常见的误解包括：集中式解决方案不具备扩展性 - 区分逻辑集中式和物理集中式，DNS 的根源包括：逻辑上集中式，物理上（大规模）分布式，跨多个组织分散式；集中式解决方案具有单点故障 - 一般情况并非如此，例如 DNS 的根源，单点故障通常更易于管理，并且更容易变得更加稳健；注意：关于可扩展性、容错性、安全性等，存在许多毫无根据的误解，我们需要培养能够轻松理解分布式系统的技能，以便判断这些误解；<br /><br />
分布式系统很复杂，从以下角度来看：架构-通用组织、流程-什么样的流程及其关系、通信-交换数据的设施、协调-独立于应用程序的算法、命名-如何识别资源？、一致性和复制-需要相同的数据的性能要求、容错性-在部分故障的情况下保持运行、安全性-确保对资源的授权访问；<br /><br />
我们想要实现什么？总体设计目标包括：支持资源共享、分发透明度、开放性、可扩展性；<br /><br />
共享资源：典型示例包括：基于云的共享存储和文件、p2p辅助多媒体流、共享邮件服务（如外包邮件系统）、共享网络托管（如内容分发网络）；观察 -“网络就是计算机”，引用当时在 Sun Microsystems 工作的 John Gage 的话；<br /><br />
分布透明度：透明度 - 分布式系统试图隐藏其进程和资源在物理上分布在多台计算机上的事实的现象，这些计算机可能相隔很远；观察 - 分布透明度通过应用程序和操作系统层（即中间件层）中的许多不同技术来处理；类型包括：访问 - 隐藏数据表示和对象访问方式的差异，位置 - 隐藏对象所在的位置，重定位 - 隐藏对象在使用时可能被移动到另一个位置，迁移 - 隐藏对象可能移动到另一个位置，复制 - 隐藏对象被复制，并发 - 隐藏对象可能由多个独立用户共享，故障 - 隐藏对象的故障和恢复；<br /><br />
透明度程度包括：(i) 旨在实现完全分布透明度可能太多：存在无法隐藏的通信延迟；完全隐藏网络和节点故障（在理论上和实践上）是不可能的，即你无法区分运行缓慢的计算机和发生故障的计算机，也无法确定服务器在崩溃前是否真正执行了操作；完全透明会降低性能，暴露系统的分布，即保持副本与主服务器完全同步需要时间，并且为了容错，会立即将写入操作刷新到磁盘；<br /><br />
(ii) 暴露分布可能太好了：利用基于位置的服务 - 找到你附近的朋友，当与不同时区的用户打交道时，当它使用户更容易了解发生了什么时 - 例如，当服务器长时间没有响应时，报告它为故障；结论 - 分布透明性是一个不错的目标，但实现它是另一回事，而且通常甚至不应该以此为目标；<br /><br />
分布式系统的开放性：开放 - 提供可轻松使用或集成到其他系统的组件的系统，开放的分布式系统本身通常由来自其他地方的组件组成；我们在说什么？能够与其他开放系统的服务进行交互，而不管底层环境如何 - 系统应符合定义良好的接口，系统应易于互操作，系统应支持应用程序的可移植性，系统应易于扩展；<br /><br />
政策与机制：政策包括：我们需要客户端数据的什么级别的一致性？我们允许下载的代码执行哪些操作？面对变化的带宽，我们要调整哪些 QoS 要求？我们需要什么级别的通信保密性？机制包括：允许（动态）设置缓存策略、支持不同级别的移动代码信任、为每个数据流提供可调整的 QoS 参数、提供不同的加密算法；<br /><br />
关于严格分离：观察 - BT 策略和机制分离越严格，我们越需要确保适当的机制，这可能会导致许多配置参数和复杂的管理；找到平衡点 - 硬编码策略通常会简化管理，并以灵活性较低为代价降低复杂性，没有明显的解决方案；<br /><br />
****
dependability: a component provides services to clients; to provide services, the component may require the services from other components -a component may depend on some other component; specifically, a component *C* depends on *C** if the correctness of *C*'s behavior depends on the correctness of *C**'s behavior, here, components are processes or channels; requirements related to dependability including: availability -readiness for usage, reliability -continuity of service delivery, safety -very low prob of catastrophes, maintainability -how easy can a failed system be repaired;<br /><br />
reliability vs. availability: reliability *R(t)* of component *C* -conditional prob that *C* has been functioning correctly during *[0,t)* given *C* was functioning correctly at time *T=0*; traditional metrics including: MTTF, MTTR, MTBF=MTTF+MTBF, here mean time to failure -the average time until a component fails, mean time to repaire -the average time needed to repair a component, mean time bt failures; terminology: failure -a component is not living up to its specifications such as crashed prog, error -part of a component that can lead to a failure such as prog bug, fault -cause of an error such as sloppy programmer, (handling faults including:) fault prevention -prevent the occurance of a fault such as do not hire sloppy programmers, fault tolerance -build a component and make it mask the occurance of a fault such as building each component by 2 independent programmers, fault removal -reduce the presence, number, or seriousness of a fault, such as getting rid of sloppy programmers, fault forecasting -estimate current presence, future incidence, and consequences of faults, such as estimating how a recruiter is doing when it comes to hiring sloppy programmers;<br /><br />
on security: observation -a distributed system that is not secure, is not dependable; what we need? confidentiality -info is disclosed only to authorized parties, integrity -ensure that alterations to assets of a system can be made only in an authorized way; authorization -verifying the correctness of a claimed identity, authentication -does an identified entity has proper access rights?, trust -one entity can be assured that another will perform particular actions according to a specific expectation;<br /><br />
security mechanisms: (i) keeping it simple: it is all about encrypting and decrypting data using security keys;<br /><br />
(ii) notation: *K(data)* denotes that we use key K to encrypt and decrypt data;<br /><br />
(iii) symmetric cryptosystem: with encryption key *E_K(data)* and decryption key *D_K(data)*, if *data=D_K(E_K(data))* then *D_K=E_K*, note: encryption and decryption key are the same and should be kept secret; asymmetric cryptosystem: distinguish a public key *PK(data)* and a private secret key *SK(data)* -encrypt msg from Alice to Bob as *data=SK_bob(data)*, sign msg for Bob by Alice as *[data,data=PK_alice(SK_alice(data))]=[data,SK_alice(data)]*;<br /><br />
(iv) secure hashing: in practice, we use secure hash functions -*H(data)* returns a fixed-length string, hence, any change from *data* to *data** will lead to a completely different string *H(data*)*, given a hash value, it is computationally impossible to find a data with *h=H(data)*;<br /><br />
(v) practical digital signatures: sign msg for Bob by Alice as *[data,H(data)=PK_alice(sgn)]=[data,H,sgn=SK_alice(H(data))]*;<br /><br />
scale in distributed systems: at least 3 components including: #users or processes -size scalability, max distance bt nodes -geographical scalability, #administrative domains -administrative scalability; observation -most systems account only for size scalability, often a solution -multiple powerful servers operating independently in parallel, today, the challenge still lies in geographical and administrative scalability;<br /><br />
size scalability: (i)root causes for scalability problems with centralized solutions including: the computational capacity, limited by cpus; the storage capacity, incorporating the transfer rate bt cpus and disks; the network bt user and centralized service;<br /><br />
(ii) [formal analysis, click to view](https://Yanhao13ai.github.io/CMU-15440/)<br /><br />
problems with geographical scalability including: cannot simply go from LAN to WAN -many distirbuted systems assume synchronous client-server interactions, where client sends req and waits for an answer, with latency may easily prohibit this scheme; WAN links are often inherently unreliable -simply moving streaming video from LAN to WAN is bound to fail; lack of multipoint comm, so that a simple search broadcast cannot be deployed, solution -develop separate naming and dir services(having their own scalability problems);<br /><br />
problems with administrative scalability: essence -conflicting policies concerning usage(and thus payment), mgmt, and security; examples including: computational grids -share expensive resources bt different domains, shared equipment -how to control, manage, and use a shared radio telescope constructed as large-scale shared sensor network?; exception -several p2p networks including: file-sharing systems -based, such as on bittorrent, p2p telephony -early versions of skype, peer-assisted audio streaming -such as spotify, note: end users collaborate and not administrative entities collaborate;<br /><br />
techniques for scaling including: hide comm latencies -make use of asynchronous comm, have separate handler for incoming response, problem -not every application fits this model; facilitate solution by moving computations to client; partition data and computations across multiple machines -move computations to clients such as java applets and scripts, decentralized naming services such as dns, decentralized info systems such as www; replication and caching by making copies of data available at different machines -replicated file servers and db, mirrored websites, web caches(in browsers and proxies), file caching(at server and client);<br /><br />
scaling is the problem with replication, applying replication is easy, except for one thing: having multiple copies(cached or replicated) leads to inconsistencies -modifying one copy makes that copy different from the rest, always keeping copies consistent and in a general way requires global sync on each modification, global sync precludes large-scale solutions; observation -if we can tolerate inconsistencies, we may reduce the need for global sync, but tolerating inconsistencies is application dependent;<br /><br />
parallel computing: observation -hp distributed computing started with parallel computing; multiprocessor and multicore vs. multicomputer: former is shared mem--interconnect--processor, latter is mem--processor--interconnect;<br /><br />
distributed shared mem systems: observation -multiprocessors are relatively easy to program in comparison to multicomputers, yet have problems when increasing #processors(or cores), solution -implement a shared-mem model on top of a multicomputer; example through vm techniques -map all main-mem pages(from different processors) into 1 single virtual addr space, if a process at processor *A* addresses a page *P* located at processor *B*, os at *A* traps and fetches *P* from *B*, just as it would if *P* had been located on local stack; problem -perf of distributed shared mem could never compete with that of multprocessors, and failed to meet the exp of programmers, it has been widely abandoned by now;<br /><br />
[cluster computing, click to view](./img/cluster-computing.png): essentially a group of high-end systems connected through a LAN, it is (i)homogeneous -same os, near-identical hardware; (ii)single, or tightly coupled managing nodes;<br /><br />
[grid computing, click for architecture](./img/grid-computing-architecture.png): the next step -plenty of nodes from everywhere, it is (i)heterogeneous; (ii)desperseed across several organizations; (iii)can easily span a wide-area network; note: to allow for collaborations, grids generally use virtual org, in essence, this is a grouping of users(or better, their IDs) that allows for authorization on resource allocation;<br /><br />
integrating applications: situation -org confronted with many networked applications, but achieving interoperability was painful; basic approach -a networked application is one that runs on a server making its services available to remote clients, simple integration -clients combine reqs for (different) applications --> send that off --> collect responses, and present a coherent result to user; next step -allow direct application-to-application comm, leading to EAI -enterprise application integration:<br /><br />
example EAI: (nested) transactions: (i) transaction -primitive&description: BEGIN_TRANSACTION -mark the start of a transaction, END_TRANSACTION -terminate the transaction and try to commit, ABORT_TRANSACTION -kill the transaction and restore the old values, READ -read data from a file, a table, or otherwise, WRITE -write data to a file, a table, or otherwise;<br /><br />
(ii) [issue -all-or-nothing, click to view](./img/enterprise_application_integration-issue.png): it is atomic -happens indivisibly(seemingly), consistent -does not violate system invariants, isolated -not mutual interference, durable -commit means changes are permanent;<br /><br />
[TPM -transaction processing monitor, click to view](./img/transaction-processing-monitor.png): observation -often the data involved in a transaction is distributed across several servers, a tp monitor is responsible for coordinating the exec of a transaction;<br /><br />
[middleware and EAI, click to view](./img/middleware-and-enterprise_application_integration.png): mw offers comm facilities for integration including: RPC -remote procedure call -reqs are sent through local procedure call, packaged as msg, processed, responded through msg, and result returned as return from call; MOM -msg oriented mw -msg are sent to logical contact point(published), and forwarded to subscribed applications;<br /><br />
how to integrate applications? including: file transfer -technically simple but not flexible, (i)figure out file format and layout, (ii)figure out file mgmt, (iii)update propagation and update notifications; shared db -much more flexible but still requires common data scheme next to risk of bottleneck; rpc -effective when exec of a series of actions is needed; messaging -rpc requires caller and callee to be up and running at the same time, messaging allows decoupling in time and space;<br /><br />
distributed pervasive systems: observation -emerging next-gen of distributed systems in which nodes are small, mobile, and often embedded in a larger system, characterized by the fact that the system naturally blends into the user's environment; 3 (overlapping) subtypes including: ubiquitous computing systems -pervasive and continuously present, i.e., there is a continuous interaction bt system and user; mobile computing systems -pervasive, but emphasis is on the fact that devices are inherently mobile; sensor (and actuator) networks -pervasive, with emphasis on the actual (collaborative) sensing and actuation of the environent;<br /><br />
ubiquitous systems: core elements including: distribution -devices are networked distributed and accessible transparently, interaction -interaction bt users and devices is highly unobtrusive, context awareness -system is aware of a user's context to optimize interaction, autonomy -devices operate autonomously without human intervention and are hence highly self-managed, intelligence -system as a whole can handle a wide range of dynamic actions and interactions;<br /><br />
[mobile computing, click to view](./img/mobile-computing.png): it is (i)a myriad of different mobile devices of smartphones, tablets, gps devices, remote controls, active badges, (ii)mobile implies that a device's location is exep to change over time, hence, change of local services, reachability, etc., keyword is discovery, (iii)maintaining stable comm can introduce serious problems, (iv)for a long time, research has focused on directly sharing resources bt mobile devices, it never became popular and is by now considered to be a fruitless path for research; bottomline -mobile devices set up connections to stationary servers, essentially bringing mobile computing in the position of clients of cloud-based services;<br /><br />
sensor networks: the nodes to which sensors are attached are many(10s-1000s), simple(small mem/compute/comm capacity), often battery-powered(or even battery-less); [sensor networks as distributed db, click for 2 extremes](./img/sensor_networks_as_distributed_db-extreme.png); [the cloud-edge continuum, click for continuum](./img/cloud-edge-continuum.png);<br /><br />
developing distributed systems: pitfalls: observation -many distributed systems are needlessly complex, caused by mistakes that required patching later on, many false assumptions are often made; false (and often hidden) assumptions including: the network is reliable, the network is secure, the network is homogeneous, the topology does not change, latency is zero, bdwidth is zero, transport cost is zero, there is 1 administrator.<br /><br />
地理可扩展性问题包括：不能简单地从 LAN 转到 WAN - 许多分布式系统假设同步客户端-服务器交互，客户端发送请求并等待答复，延迟可能很容易阻止这种方案；WAN 链接通常本质上不可靠 - 简单地将流视频从 LAN 移动到 WAN 注定会失败；缺乏多点通信，因此无法部署简单的搜索广播，解决方案 - 开发单独的命名和目录服务（具有自己的可扩展性问题）；<br /><br />
管理可扩展性问题：本质 - 有关使用（以及付款）、管理和安全性的冲突政策；示例包括：计算网格 - 在不同域、共享设备之间共享昂贵的资源 - 如何控制、管理和使用构建为大规模共享传感器网络的共享射电望远镜？；例外 - 几个 p2p 网络包括：基于文件共享系统，例如 bittorrent、p2p 电话 -skype 的早期版本、对等辅助音频流 -例如 spotify，注意：最终用户协作而不是管理实体协作；<br /><br />
扩展技术包括：隐藏通信延迟 - 使用异步通信，对传入响应有单独的处理程序，问题 - 并非每个应用程序都适合此模型；通过将计算移动到客户端来促进解决方案；在多台机器上划分数据和计算 - 将计算移动到客户端，例如 java 小程序和脚本、分散命名服务（例如 dns）、分散信息系统（例如 www）；通过在不同的机器上提供数据副本来实现复制和缓存 - 复制文件服务器和数据库、镜像网站、Web 缓存（在浏览器和代理中）、文件缓存（在服务器和客户端）；<br /><br />
扩展是复制的问题，应用复制很容易，但有一点除外：拥有多个副本（缓存或复制）会导致不一致 - 修改一个副本会使该副本与其他副本不同，始终保持副本一致，并且通常需要对每次修改进行全局同步，全局同步会排除大规模解决方案；观察 - 如果我们能够容忍不一致，我们可能会减少对全局同步的需求，但容忍不一致取决于应用程序；<br /><br />
并行计算：观察 -hp 分布式计算始于并行计算；多处理器和多核与多计算机：前者是共享内存-互连-处理器，后者是内存-处理器-互连；<br /><br />
分布式共享内存系统：观察 - 与多计算机相比，多处理器相对容易编程，但在增加处理器（或核心）数量时会出现问题，解决方案 - 在多计算机之上实现共享内存模型；例如通过虚拟机技术 - 将所有主内存页面（来自不同的处理器）映射到 1 个虚拟地址空间，如果处理器 *A* 上的进程寻址位于处理器 *B* 上的页面 *P*，则 *A* 上的操作系统会从 *B* 捕获并获取 *P*，就像 *P* 位于本地堆栈上一样；问题 - 分布式共享内存的性能永远无法与多处理器相媲美，也无法满足程序员的需要，因此现在已被广泛抛弃；<br /><br />
[集群计算，点击查看](./img/cluster-computing.png)：本质上是一组通过 LAN 连接的高端系统，它是 (i) 同质的 - 相同的操作系统，几乎相同的硬件；(ii) 单一或紧密耦合的管理节点；<br /><br />
[网格计算，点击查看架构](./img/grid-computing-architecture.png)：下一步 - 来自各处的大量节点，它是 (i) 异构的；(ii) 分散在多个组织中；(iii) 可以轻松跨越广域网；注意：为了实现协作，网格通常使用虚拟组织，本质上，这是一组用户（或更好的是他们的 ID）的分组，允许对资源分配进行授权；<br /><br />
集成应用程序：情况 - 组织面临许多联网应用程序，但实现互操作性却很痛苦；基本方法 - 联网应用程序是在服务器上运行的应用程序，使其服务可供远程客户端使用，简单集成 - 客户端组合（不同）应用程序的请求 --> 发送 --> 收集响应，并向用户呈现连贯的结果；下一步 - 允许直接应用程序到应用程序通信，从而实现 EAI - 企业应用程序集成：<br /><br />
示例 EAI：（嵌套）事务：(i) 事务 - 原始和说明：BEGIN_TRANSACTION - 标记事务的开始，END_TRANSACTION - 终止事务并尝试提交，ABORT_TRANSACTION - 终止事务并恢复旧值，READ - 从文件、表或其他位置读取数据，WRITE - 将数据写入文件、表或其他位置；<br /><br />
(ii) [问题 - 全有或全无，单击查看](./img/enterprise_application_integration-issue.png)：它是原子 -不可分割地发生（表面上），一致 -不违反系统不变量，隔离 -不相互干扰，持久 -提交意味着更改是永久性的；<br /><br />
[TPM -事务处理监视器，单击查看](./img/transaction-processing-monitor.png)：观察 -通常，事务中涉及的数据分布在多个服务器上，TP监视器负责协调事务的执行；<br /><br />
[中间件和EAI，单击查看](./img/middleware-and-enterprise_application_integration.png)：中间件为集成提供通信设施，包括：RPC -远程过程调用 -请求通过本地过程调用发送，打包为消息，处理，通过消息响应，并将结果作为调用返回； MOM - 面向消息的中间件 - 消息被发送到逻辑接触点（已发布），并转发给订阅的应用程序；<br /><br />
如何集成应用程序？包括：文件传输 - 技术上简单但不灵活，（i）弄清楚文件格式和布局，（ii）弄清楚文件管理，（iii）更新传播和更新通知；共享数据库 - 更灵活，但仍然需要通用数据方案，否则存在瓶颈风险；rpc - 在需要执行一系列操作时有效；消息传递 -rpc 要求调用者和被调用者同时启动和运行，消息传递允许在时间和空间上解耦；<br /><br />
分布式普适系统：观察 - 新兴的下一代分布式系统，其中节点很小、可移动，并且通常嵌入在更大的系统中，其特点是系统自然融入用户的环境； 3 种（重叠）子类型包括：普适计算系统 - 无处不在且持续存在，即系统和用户之间存在持续的交互；移动计算系统 - 无处不在，但重点在于设备本质上是移动的；传感器（和执行器）网络 - 无处不在，重点是环境的实际（协作）感知和驱动；<br /><br />
无处不在的系统：核心元素包括：分布 - 设备联网分布且透明可访问，交互 - 用户和设备之间的交互非常不引人注目，情境感知 - 系统了解用户情境以优化交互，自主性 - 设备自主运行而无需人工干预，因此高度自我管理，智能 - 整个系统可以处理各种动态操作和交互；<br /><br />
[移动计算，单击查看](./img/mobile-computing.png)：它是 (i) 智能手机、平板电脑、gps 设备、遥控器、活动徽章等各种移动设备，(ii) 移动意味着设备的位置可以随时间变化，因此本地服务、可达性等会发生变化，关键词是发现，(iii) 维持稳定的通信可能会引发严重问题， (iv)长期以来，研究一直专注于直接通过移动设备共享资源，但这种做法从未流行起来，目前被认为是一条徒劳的研究之路；底线——移动设备与固定服务器建立连接，本质上将移动计算置于基于云的服务的客户端位置；<br /><br />
传感器网络：传感器连接的节点很多（10-1000 个），简单（内存/计算/通信容量小），通常由电池供电（甚至无电池）；[传感器网络作为分布式数据库，单击可查看 2 个极端](./img/sensor_networks_as_distributed_db-extreme.png)；[云边缘连续体，单击可查看连续体](./img/cloud-edge-continuum.png)；<br /><br />
开发分布式系统：陷阱：观察——许多分布式系统不必要地复杂，这是由需要稍后修补的错误引起的，通常会做出许多错误的假设；错误的（通常是隐藏的）假设包括：网络是可靠的、网络是安全的、网络是同质的、拓扑结构不变、延迟为零、带宽为零、传输成本为零、有 1 个管理员。<br /><br />
****
### Architectures
architectural styles: a style is formulated in terms of: (replaceable) components with well-defined interfaces, the way that components are connected to each other, the data exchanged bt components, how these components and connectors are jointly configured into a system; here, a connector is a mechanism that mediates comm, coordination, or cooperation among components, examples including: facilities for rpc, messaging, or streaming;<br /><br />
[layered architecture style, click to view](./img/layered-architecture-style.png)<br /><br />
example: [comm protocols, click to view](./img/comm-protocols.png): [two-party commu, click to view](./img/two-party-comm.png);<br /><br />
application layering: traditional three-layered view incorporating: application-interface layer -contains units for interfacing to users or external applications, processing layer -contains the functions of an application, i.e., without specific data, data layer -contains the data that a client wnants to manipulate through the application components; observation -this layering is found in many distributed info systems, using traditional db technology and accompanying applications;<br /><br />
example: [simple search engine, click to view](./img/simple-search-engine.png);<br /><br />
[object-based style, click to view](./img/object-based-style.png): essence -components are objects, connected to each other through procedure calls, objects may be placed on different machines, calls can hence execute across a network; encapsulation -objects are said to encapsulate data and offer methods on that data without revealing the internal implementation;<br /><br />
RESTful architectures: essence -view a distributed system as a collection of resources, individually managed by components, resources may be added, removed, retrieved, and modified by (remote) applications, hence: resources are identified through a single naming scheme, all services offer the same interface, msg sent to or from a service are fully self-described, after executing an op at a service, that component forgets everything about the caller; basic ops including: PUT -create a new resource, GET -retrieve the state of a resource in some representation, DELETE -delete a resource, POST -modify a resource by transferring a new state;<br /><br />
example: amazon's single storage service: essence -objects i.e., files, are placed into buckets i.e., dirs, here: buckets cannot be placed into buckets, ops on ObjectName in bucket BucketName require the following identifier: http://BucketName.s3.amazonaws.com/ObjectName; typical ops(all ops are carried out by sending http reqs): create a bucket or object -PUT, along with the URI, listing objects -GET, on a bucket name, reading an object -GET, on a full URI;<br /><br />
on interfaces: issue -many people like restful approaches because the interface to a service is so simple, the catch is that much needs to be done in the param space; [amazon s3 SOAP interface, click to view](./img/amazon-s3-soap-interface.png); simplications -assume an interface *bucket* offering an op *create*, requiring an input string such as *mybucket*, for creating a bucket "mybucket":<br /><br />
SOAP
```
import bucket
bucket.create("mybucket")
```
RESTful
```
PUT "http://mybucket.s3.amazonaws.com/"
```
[coordination, click to view](./img/coordination.png)<br /><br />
example: [linda tuple space, click to view](./img/linda-tuple-space.png): 3 simple ops including: *in(t)* -remove a tuple matching template *t*, *rd(t)* -obtain copy of a tuple matching template *t*, *out(t)* -add tuple *t* to the tuple space; more details including: calling *out(t)* twice in a row, leads to storing 2 copies of tuple, hence, a tuple space is modeled as a multiset; both *in* and *rd* are blocking ops -the caller will be blocked until a matching tuple is found, or has become available;<br /><br />
publish and subscribe: issue -[how to match events? click to view](./img/publish-and-subscribe.png): assume events are described by (attribute, value) pairs, topic-based subscription -specify a "attribute=value" series, content-based subscription -specify a "attribute belongs to range" series; observation -content-based subscriptions may easily have serious scalability problems;<br /><br />
[mw: the os of distributed systems, click to view](./img/middleware.png): what does it contain? commonly used components and functions that need not be implemented by applications separately; using latency to build mw, problem -the interfaces offered by a legacy component are most likely not suitable for all applications, solution -a wrapper or adaptor offers an interface acceptable to a client application, its functions are transformed into those available at the component, [click for organizing wrappers](./img/organizing-wrappers.png); developing adaptable mw, problem -mw contains solutions that are good for most applications, hence, you may want to adapt its behavior for specific applications;<br /><br />
[intercept the usual flow of control, click to view](./img/usual-flow-of-control.png);<br /><br />
centralized system architectures including: [basic client-server model, click to view](./img/client-server-model.png), it is (i)there are processes offering services called servers, (ii)there are processes that use service called clients, (iii)clients and servers can be on different machines, (iv)clients follow req/reply model regarding using services;<br /><br />
multitiered centralized system architectures including: single-tiered, two-tiered, three-tiered, [click for traditional two-tiered config](./img/two-tiered-centralized-system-config.png), [click to view three-tiered](./img/three-tiered-centralized-system-model.png);<br /><br />
example: [NFS -network file system, click to view](./img/nfs.png): foundations -each nfs server provides a standardized view of its local file system, i.e., each server supports the same model regardless the implementation of file system, [click for nfs remote access model](./img/nfs-remote-access-model.png), note: ftp is a typical upload/download model, the same can be said for systems like dropbox;<br /><br />
example: simple web servers, [click for back in the old days ones](./img/simple-web-servers.png): life was simple by (i)a website consisted as a collection of html files, (ii)html files could be referred to each other by a hyperlink, (iii)a web server essentially needed only a hyperlink to fetch a file, (iv)a browser took care of properly rendering the content of a file; less simple web servers, [click for still back in the old days ones](./img/less-simple-web-servers.png): life became a bit more complicated by (i)a website was built around a db with content, (ii)a webpage could still be referred to by a hyperlink, (iii)a web server essentially needed only a hyperlink to fetch a file, (iv)a separate prog (Common Gateway Interface) composed a page, (v)a browser took care of properly rendering the content of a file;<br /><br />
alternative orgs including: vertical distribution -comes from dividing distributed applications into 3 logical layers, and running the components from each layer on a different server(machine); horizontal distribution -a client or server may be physically split up into logically equivalent parts, but each part is operating on its own share of the complete dataset; p2p architectures -processes are all equal, i.e., the functions that need to be carried out are represented by every process, hence, each process will act as a client and a server at the same time(i.e., acting as a servant);<br /><br />
structured p2p: essence -make use of a semantic-free index, i.e., each data item is uniquely associated with a key, in turn used as an index, common practice -use a hash function *key(data item)=hash(data item's value)*, p2p system now responsible for storing (key,value) pairs;<br /><br />
example: [hypercube, click to view](./img/hypercube.png);<br /><br />
example: [chord, click to view](./img/chord.png): nodes are logically organized in a ring, each node has an m-bit identifier; each data item is hashed to an m-bit key; data item with key k is stored at node with smallest identifier id>=k, called the successor of key k; the ring is extended with various shortcut links to other nodes;<br /><br />
unstructured p2p: essence -each node maintains an ad hoc list of neighbors, the resulting overlay resembles a random graph -an edge <u,v> exists only with a certain prob P[<u,v>]; searching by (i)flooding -issuing node u passes req for d to all neighbors, req is ignored when receiving node had seen it before, otherwise, v searches locally for d(recursively), may be limited by a time-to-live -a max #hops, (ii)random walk -issuing node u passes req for d to randomly chosen neighbor v, if v does not have d, it forwards req to one of its randomly chosen neighbors, and so on; flooding vs. random walk: [formal analysis, click to view](https://Yanhao13ai.github.io/CMU-15440/)<br /><br />
super-peer networks: essence -it is sometimes sensible to break the symmetry in pure p2p networks when searching in unstructured p2p systems, having index servers improves perf, and when deciding where to store data can often be done more efficiently through brokers, [click to view](./img/super-peer-networks.png);<br /><br />
[collaboration: the bittorrent case: principle -search for a file F, click to view](./img/collaboration-bittorrent-search-for-a-file.png): lookup file at a global dir, hence, returns a torrent file; torrent file contains reference to tracker -a server keeping an accurate account of active nodes that have (chunks of) F; P can join swarm, get a chunk for free, and then trade a copy of that chunk for another one with a peer Q also in the swarm;<br /><br />
[cloud computing, click to view](./img/cloud-computing.png): make a distinction bt 4 layers including: hw -processors, routers, power and cooling systems, customers normally never get to see these; infra -deploys virtualization techniques, evolves around allocating and managing virtual storage devices and virtual servers; platform -provides higher-level abstractions for storage and such, such as amazon s3 storage system offers an api for (locally created) files to be organized and stored in so-called buckets; application -actual applications, such as office suites(text processors, spreadsheet applications, presentation applications), comparable to the suite of applications shipped with OSes;<br /><br />
[edge-server architecture, click to view](./img/edge-server.png): essence -systems deployed on the internet where servers are placed at the edge of the network -the boundary bt enterprise networks and the actual internet; reasons for having an edge infra including: latency and bdwidth -especially important for certain realtime applications such as ar/vr applications, many people underestimates the latency and bdwidth to the cloud; reliability -the connection to the cloud is often assumed to be unreliable, which is often a false assumption, there may be critical situations in which extremely high connectivity guarantees are needed; security and privacy -the implicit assumption is often that when assets are nearby, they can be made better protected, practice shows that this assumption is generally false, however, securely handling data ops in the cloud may be trickier than within your own org; edge orchestration -managing resources at the edge may be trickier than in the cloud in terms of: resource allocation -we need to guarantee availability of the resources required to perform a service; service placement -we need to decide when and where to place a service, this is notably relevant for mobile applications; edge selection -we need to decide which edge infra should be used when a service needs to be offered, the closest one may not be the best one; observation -there is still a lot of buzz about edge infra and computing, yet whether all that buzz makes any senses remains to be seen.<br /><br />
架构风格：风格由以下方面构成：具有明确定义接口的（可替换）组件、组件相互连接的方式、组件之间交换的数据、这些组件和连接器如何共同配置成一个系统；此处，连接器是一种在组件之间进行通信、协调或合作的机制，示例包括：rpc、消息传递或流式传输设施；<br /><br />
[分层架构风格，点击查看](./img/layered-architecture-style.png)<br /><br />
示例：[通信协议，点击查看](./img/comm-protocols.png)：[双方通信，点击查看](./img/two-party-comm.png)；<br /><br />
应用分层：传统的三层视图，包括：应用接口层 - 包含用于与用户或外部应用程序交互的单元，处理层 - 包含应用程序的功能，即没有特定数据，数据层 - 包含客户端希望通过应用程序组件操作的数据；观察 - 这种分层存在于许多分布式信息系统中，使用传统的数据库技术和附带的应用程序；<br /><br />
示例：[简单搜索引擎，点击查看](./img/simple-search-engine.png)；<br /><br />
[基于对象的样式，点击查看](./img/object-based-style.png)：本质 - 组件是对象，通过过程调用相互连接，对象可以放置在不同的机器上，因此调用可以通过网络执行；封装 - 对象被说成是封装数据并提供针对该数据的方法，而不暴露内部实现；<br /><br />
RESTful 架构：本质 - 将分布式系统视为资源的集合，由组件单独管理，资源可以由（远程）应用程序添加、移除、检索和修改，因此：资源通过单一命名方案进行标识，所有服务都提供相同的接口，发送到服务或从服务发出的消息是完全自我描述的，在服务上执行操作后，该组件将忘记有关调用者的一切；基本操作包括：PUT -创建新资源，GET -以某种形式检索资源状态，DELETE -删除资源，POST -通过传输新状态修改资源；<br /><br />
示例：亚马逊的单一存储服务：本质 -对象，即文件，被放入存储桶，即目录中，这里：存储桶不能放入存储桶，存储桶 BucketName 中 ObjectName 上的操作需要以下标识符：http://BucketName.s3.amazonaws.com/ObjectName；典型操作（所有操作都通过发送 http 请求来执行）：创建存储桶或对象 -PUT，以及 URI，列出对象 -GET，在存储桶名称上，读取对象 -GET，在完整 URI 上；<br /><br />
关于接口：问题 -许多人喜欢 restful 方法，因为服务的接口非常简单，问题是需要在参数空间中做很多事情； [amazon s3 SOAP接口，点击查看](./img/amazon-s3-soap-interface.png);简化 - 假设接口 *bucket* 提供操作 *create*，需要输入字符串（例如 *mybucket*）来创建存储桶“mybucket”：<br /><br />
SOAP
```
import bucket
bucket.create("mybucket")
```
RESTful
```
PUT "http://mybucket.s3.amazonaws.com/"
```
[coordination，单击查看](./img/coordination.png)<br /><br />
示例：[linda tuple space，单击查看](./img/linda-tuple-space.png)：3 个简单操作包括：*in(t)* -删除与模板 *t* 匹配的元组，*rd(t)* -获取与模板 *t* 匹配的元组的副本，*out(t)* -将元组 *t* 添加到元组空间；更多细节包括：连续两次调用*out(t)*，导致存储2个tuple副本，因此tuple空间被建模为多集；*in*和*rd*都是阻塞操作 - 调用者将被阻塞，直到找到匹配的tuple，或者tuple可用；<br /><br />
发布和订阅：问题 -[如何匹配事件？点击查看](./img/publish-and-subscribe.png)：假设事件由（属性，值）对描述，基于主题的订阅 - 指定“属性=值”系列，基于内容的订阅 - 指定“属性属于范围”系列；观察 - 基于内容的订阅可能容易出现严重的可扩展性问题；<br /><br />
[mw：分布式系统的操作系统，点击查看](./img/middleware.png)：它包含什么？常用的组件和功能，不需要由应用程序单独实现；使用延迟来构建 mw，问题 - 遗留组件提供的接口很可能不适合所有应用程序，解决方案 - 包装器或适配器提供客户端应用程序可接受的接口，其功能被转换为组件上可用的功能，[单击以组织包装器](./img/organizing-wrappers.png)；开发适应性 mw，问题 - mw 包含适用于大多数应用程序的解决方案，因此，您可能希望针对特定应用程序调整其行为；<br /><br />
[拦截通常的控制流，点击查看](./img/usual-flow-of-control.png);<br /><br />
集中式系统架构包括：[基本客户端-服务器模型，点击查看](./img/client-server-model.png)，它是 (i) 有提供服务的进程称为服务器，(ii) 有使用服务的进程称为客户端，(iii) 客户端和服务器可以位于不同的机器上，(iv) 客户端在使用服务时遵循请求/回复模型；<br /><br />
多层集中式系统架构包括：单层、双层、三层，[点击查看传统的双层配置](./img/two-tiered-centralized-system-config.png)，[点击查看三层](./img/three-tiered-centralized-system-model.png);<br /><br />
示例：[NFS - 网络文件系统，点击查看视图](./img/nfs.png): 基础 - 每个 nfs 服务器都提供其本地文件系统的标准化视图，即，无论文件系统的实现如何，每个服务器都支持相同的模型，[单击查看 nfs 远程访问模型](./img/nfs-remote-access-model.png)，注意：ftp 是一种典型的上传/下载模型，对于像 dropbox 这样的系统来说也是如此；<br /><br />
示例：简单的 Web 服务器，[单击查看过去的 Web 服务器](./img/simple-web-servers.png)：生活很简单，因为 (i) 网站由 html 文件集合组成，(ii) html 文件可以通过超链接相互引用，(iii) Web 服务器基本上只需要超链接即可获取文件，(iv) 浏览器负责正确呈现文件的内容；不那么简单的网络服务器，[点击查看旧时代的服务器](./img/less-simple-web-servers.png)：生活变得有点复杂，因为 (i) 网站是围绕包含内容的数据库构建的，(ii) 网页仍然可以通过超链接引用，(iii) 网络服务器基本上只需要超链接即可获取文件，(iv) 单独的程序 (通用网关接口) 组成页面，(v) 浏览器负责正确呈现文件的内容；<br /><br />
替代组织包括：垂直分布 - 将分布式应用程序划分为 3 个逻辑层，并在不同的服务器（机器）上运行每个层的组件；水平分布 - 客户端或服务器可能在物理上被分成逻辑上等效的部分，但每个部分都在完整数据集的自己份额上运行； p2p 架构 - 进程都是平等的，即每个进程都代表需要执行的功能，因此每个进程将同时充当客户端和服务器（即充当仆人）；<br /><br />
结构化 p2p：本质 - 使用无语义索引，即每个数据项都与一个键唯一关联，反过来用作索引，常见做法 - 使用哈希函数 *key(数据项)=hash(数据项的值)*，p2p 系统现在负责存储 (key,value) 对；<br /><br />
示例：[hypercube，单击查看](./img/hypercube.png);<br /><br />
示例：[chord，单击查看](./img/chord.png)：节点在逻辑上组织成环，每个节点都有一个 m 位标识符；每个数据项被哈希为一个 m 位密钥；密钥为 k 的数据项存储在标识符为 id>=k 的最小节点，称为密钥 k 的后继节点；环通过各种快捷方式链接到其他节点进行扩展；<br /><br />
非结构化 p2p：本质 - 每个节点维护一个邻居的临时列表，生成的覆盖图类似于随机图 - 边 <u,v> 仅以某个概率 P[<u,v>] 存在；通过 (i) 泛洪搜索 - 发出节点 u 将 d 的请求传递给所有邻居，如果接收节点之前已经看到过，则忽略该请求，否则，v 会在本地（递归）搜索 d，可能会受到生存时间的限制 - 最大跳数，(ii) 随机游走 - 发出节点 u 将 d 的请求传递给随机选择的邻居 v，如果 v 没有 d，它会将请求转发给其随机选择的邻居之一，依此类推；洪泛与随机游走：[正式分析，点击查看](https://Yanhao13ai.github.io/CMU-15440/)<br /><br />
超级对等网络：本质 - 在非结构化 p2p 系统中搜索时，打破纯 p2p 网络中的对称性有时是明智的，拥有索引服务器可以提高性能，并且当决定将数据存储在哪里时，通常可以通过代理更有效地完成，[点击查看](./img/super-peer-networks.png);<br /><br />
[协作：bittorrent 案例：原理 - 搜索文件 F，点击查看](./img/collaboration-bittorrent-search-for-a-file.png)：在全局目录中查找文件，因此返回一个 torrent 文件；torrent 文件包含对跟踪器的引用 - 跟踪器是一个服务器，它准确记录了具有（块）F 的活动节点； P 可以加入 Swarm，免费获得一个块，然后与 Swarm 中的对等 Q 交换该块的副本；<br /><br />
[云计算，点击查看](./img/cloud-computing.png)：区分 4 层，包括：硬件 - 处理器、路由器、电源和冷却系统，客户通常看不到这些；基础设施 - 部署虚拟化技术，围绕分配和管理虚拟存储设备和虚拟服务器而发展；平台 - 为存储等提供更高级别的抽象，例如亚马逊 s3 存储系统提供 api 用于组织和存储在所谓的存储桶中的（本地创建的）文件；应用程序 - 实际应用程序，例如办公套件（文本处理器、电子表格应用程序、演示应用程序），可与操作系统附带的应用程序套件相媲美；<br /><br />
[edge-server 架构，单击查看](./img/edge-server.png)：本质 - 部署在互联网上的系统，服务器位于网络边缘 - 企业网络和实际互联网的边界；拥有边缘基础设施的原因包括：延迟和带宽 - 对于某些实时应用程序（例如 ar/vr 应用程序）尤其重要，许多人低估了到云的延迟和带宽；可靠性 - 与云的连接通常被认为是不可靠的，这通常是一个错误的假设，在危急情况下可能需要极高的连接保证；安全和隐私——隐含的假设通常是，当资产在附近时，可以得到更好的保护，实践表明这种假设通常是错误的，然而，在云中安全地处理数据操作可能比在自己的组织内更棘手；边缘编排——在边缘管理资源可能比在云中更棘手：资源分配——我们需要保证执行服务所需资源的可用性；服务放置——我们需要决定何时何地放置服务，这对于移动应用程序尤其重要；边缘选择——我们需要决定在需要提供服务时应该使用哪种边缘基础设施，最近的可能不是最好的；观察——关于边缘基础设施和计算仍然有很多讨论，但所有这些讨论是否有意义还有待观察。<br /><br />
****
### Processes
threads: we build virtual processors in sw, on top of physical processors, here, processor -provides a set of instr along with the capability of automatically executing a series of those instr, thread -a minimal sw processor in whose context a series of instr can be executed, saving a thread context implies stopping the current exec and saving all the data needed to continue the exec at a later stage, process -a sw processor in whose context one or more threads may be executed, executing a thread, means executing a series of instr in the context of that thread;<br /><br />
context switching: (i) contexts including: processor context -the minimal collection of values stored in the regs of a processor used for exec of a series of instr, for example: sp, addressing regs, pc; thread context -the minimal collection of values stored in regs and mem, used for exec of a series of instr, i.e., processor context, state; process context -the minimal context of values stored in regs and mem, used for exec of a thread, i.e., thread context, but now also at least mmu reg values;<br /><br />
(ii) observations including: threads share the same addr space, thread context switching can be done entirely independent of os; process switching is generally (somewhat) more expensive as it involves getting os in the loop, i.e., trapping to the kernel; creating and destroying threads is much cheaper than doing so for processes;<br /><br />
why use threads? reasons including: avoid needless blocking -a single-threaded process will block when doing io, in a multithreaded process, os can switch the cpu to another thread in that process; exploit parallelism -the threads in a multithreaded process can be scheduled to run in parallel on a multiprocessor or multicore processor; [avoid process switching, click to view](./img/avoid-process-switching.png) -structure large applications not as a collection of processes, but through multiple threads, trade-offs including: threads use the same addr space -more prone to errors, no support from os/hw to protect threads using each other's mem, thread context switching may be faster than process context switching;<br /><br />
cost of a context switching: consider a simple clock-interrupt handler incorporating: direct costs -actual switch and executing code of the handler, indirect costs -other costs notably caused by messing up the cache; what a context switching may cause is [indirect costs, click to view](./img/context-switching-indirect-costs.png), [click for a simple example in python](./img/context-switching-example.png),[[2]](./img/context-switching-example-2.png),[[3]](./img/context-switching-example-3.png);<br /><br />
threads and os: main issue -should an os kernel provide threads, or should they be implemented as user-level packages? user-space solution -all ops can be completely handled within a single process hence implementations can be extremely efficient, all services provided by the kernel are done on behalf of the process in which a thread resides hence if the kernel decides to block a thread then the entire process will be blocked, threads are used when there are many external events -threads block on a per-event basis, hence if the kernel cannot distinguish threads, how can it support signaling events to them?; kernel-solution -the whole idea is to have the kernel contain the implementation of a thread package, meaning that all ops return as system calls, i.e., ops that block a thread are no longer a problem when kernel schedules another available thread within the same process, handling external events is simple when kernel(which catches all events) schedules the thread associated with the event, the problem is(or used to be) loss of efficiency because each thread op requires a trap to the kernel; conclution -but, try to mix user-level and kernel threads into a single concept, however, perf gains has not turned out to generally outweigh the increased complexity, here, [combining user-level and kernel threads basic idea is to introduce a two-level threading approach -kernel threads that can execute user-level threads, click to view](./img/combining-userlevel-and-kernel-threads.png), here, user-level and kernel threads combined principle ops including: user-level thread does system call, hence, the kernel thread that is executing that user thread, blocks, user-level thread remains bound to the kernel thread; the kernel can schedule another kernel thread having a runnable user thread bound to it, note: this user-level thread can switch to any other runnable user-level thread currently in user space; a user-level thread calls a blocking user-level op, hence, do context switching to a runnable user-level thread(then bound to the same kernel thread); when there are no user-level threads to schedule, a kernel thread may remain idle, and may even be removed(destroyed) by the kernel;<br /><br />
using threads at the client side: (i) multithreaded web client: hiding network latencies -web browser scans an incoming html page, and finds that more files need to be fetched; each file is fetched by a separate thread, each doing a (blocking) http req; as files come in, browser displays them;<br /><br />
(ii) multiple req-response calls to other machines (rpc): a client does several calls at the same time, each one by a different thread; it then waits until all results have been returned; note: if calls are to different servers, we may have a linear speed-up;<br /><br />
(iii) multithreaded clients -does it help? TLP -thread-level parallelism: let *ci* denote the fraction of time that exactly *i* threads are being executed simultaneously, *TLP=sigma_{i=1}{N}i*ci / 1-c0*, here, *N* is max #threads that (can) execute at the same time; practical measurements -a typical web browser has a TLP value bt 1.5-2.5, hence, threads are primarily used for logically organizing browsers;<br /><br />
using threads at the server side: (i) improve perf: starting a thread is cheaper than starting a new process; having a single-threaded server prohibits simple scale-up to a multiprocesser system; as with clients -hide network latency by reacting to next req while previous one is being replied;<br /><br />
(ii) better structure: most servers have high io demands, using simple well-understood blocking calls simplifies the structure; multithreaded prog tend to be smaller and easier to understand due to simplified flow of control;<br /><br />
why multithreading is popular -org: [dispatcher/worker model, click to view](./img/dispatcher-worker-model.png); overview(model&characteristics) -multithreading -parallelism, bocking system calls, single-threaded process -no parallelism, blocking system calls, finite-state machine -parallelism, nonblocking system calls;<br /><br />
virtualization: (i) observation -virtualization is important because: hw changes faster than sw, ease of portability and code migration, isolation of failing or attacked components;<br /><br />
(ii) principle -[mimicking interfaces, click to view](./img/virtualization-mimicking-interfaces.png), here, 4 types of interfaces at 3 different levels: instr set architecture -the set of machine instr, with 2 subsets of: privileged instr -allowed to be executed only by os, general instr -can be executed by any prog; system calls as offered by an os; lib calls, known as an api;<br /><br />
(iii) [ways of virtualization, click to view](./img/ways-of-virtualization.png), here, diffs lie in: process vm is separate set of instr, an interpreter/emulator, running atop an os; native vmm is low-level instr, along with bare-bones minimal os; hosted vmm is low-level instr, but delegating most work to a full-fledged os;<br /><br />
(iv) zooming into vms :perf: [refining the org, click to view](./img/vm-performance-refining-organizations.png); special instr including: control-sensitive instr -may affect config of a machine such as one affecting relocation reg or interrupt table, behavior-sensitive instr -effect is partially determined by context such as *POPF* sets an interrupt-enabled flag, but only in system mode;<br /><br />
(v) condition for virtualization: necessary condition -for any conventional computer, a vm monitor may be constructed if the set of sensitive instr for that computer is a subset of the set of priviledged instr; problem :condition is not always satisfied -there may be sensitive instr that are executed in user mode without causing a trap to os; solution -emulate all instr, wrap nonpriviledged sensitive instr to divert control to vmm, paravirtualization -modify guest os either by preventing nonpriviledged sensitive instr or making them nonsensitive(i.e., changing the context);<br /><br />
(vi) [containers, click to view](./img/containers.png): namespaces -a collection or processes in a container is given their own view of identifiers, union file system -combine several file systems into a layered fashion with only the highest layer allowing for *write* ops(and the one being part of a container), control groups -resource restrictions can be imposed upon a collection of processes; example :[planetlab, click to view](./img/containers-example-planetlab.png): essence -different org contribute machines, which they subsequently share for various experiments, problem -we need to ensure that different distributed applications do not get into each other's way, hence, virtualization, Vserver -independent and protected environment with its own libs, server versions, etc., distributed applications are assigned a collection of vservers distributed across multiple machines; planetlab vservers and slices: essence -each vserver operates in its own environment cf. *chroot*, linux enhancements include proper adjustment of process IDs such as *init* having ID0, 2 processes in different vservers may have same user ID but does not imply the same user, [separation leads to slice -click to view](./img/vserver-separation-leads-to-slices.png);<br /><br />
(vii) vms and cloud computing: 3 types of cloud services including: IaaS covering the basic infra, PaaS covering system-level services, SaaS containing actual applications, here, IaaS -instead of renting out a physical machine, a cloud provider will rent out a vm(or vmm) that may be sharing a physical machine with other customers, hence, almost complete isolation bt customers(although perf isolation may not be reached); [client-server interaction, click for distinguishing application-level and mw-level solutions](./img/distinguish-application-level-and-middleware-level-solutions.png), example :[X Window system, click to view](./img/x-window-system.png), here there is X client and server -application acts as a client to the X-kernel, the latter running as a server on the client's machine, moreover, improving X by: practical observations -there is often no clear separation bt application logic and ui commands, applications tend to operate in a tightly synchornous manner with an X-kernel, alternative approaches including: let applications control the display completely up to the pixel level such as VNC, provide only a few high-level display ops(dependent on local video drivers) allowing more efficient display ops;<br /><br />
(viii) virtual desktop environment: logical dev -with an increasing #cloud-based applications, the question is how to use those applications from a user's premise? issue -develop th ultimate networked ui, answer -use a web browser to establish a seamless experience, [click for the anatomy of a web browser](./img/anatomy-of-a-web-browser.png);<br /><br />
client-side sw :generally tailored for distribution transparency: access transparency -client-side stubs for rpcs; location/migration transparency -let client-side sw keep track of actual location; [replication transparency -multiple invocations handled by client stub, click to view](./img/client-side-software-replication-transparency.png); failure transparency -can often be placed only at client(we are trying to mask server and comm failures);<br /><br />
servers: (i) general org: basic model -a process implementing a specific service on behalf of a collection of clients, it waits for an incoming req from a client and subsequently ensures that the req is taken care of, after which it waits for the next incoming req; 2 basic types of: iterative server -server handles req before attending a next req, concurrent server -uses a dispatcher, which picks up an incoming req that is then passed onto a separate thread/process; observation -concurrent servers are the norm that they can easily handle multiple req, notably in the presence of blocking ops(to disks or other servers);<br /><br />
(ii) contacting a server: observation -[most services are tied to a specific port, click to view](./img/contacting-a-server.png); [click for dynamically assigning an endpoint through 2 approaches](./img/dynamically-assigning-an-endpoint-through-two-approaches.png);<br /><br />
(iii) out-of-band comm: issue -is it possible to interrupt a server once it has accepted(or is in process of accepting) a service req? solution1 -use a separate port for urgent data, now, server has a separate thread/process for urgent msg, urgent msg comes in hence associated req is put on hold, note: we require os supports priority-based scheduling; solution2 -use facilities of the transport layer, for example: tcp allows for urgent msg in same connection, now, urgent msg can be caught using os signaling techniques;<br /><br />
(iv) servers and state: stateless servers -never keep accurate info about the status of a client after having handled a req, now, do not record whether a file has been opened(simply close it again after access), do not promise to invalidate a client's cache, do not keep track of your clients; consequences -clients and servers are completely independent, state inconsistencies due to client and server crashes are reduced, possible loss of perf because such as a server cannot acticipate client behavior(think of fetching file blocks); question -does connection-oriented comm fit into a stateless design?; stateful server -keeps track of status of its clients, now, record that a file has been opened so that prefetching can be done, knows which data a client has cached and allows clients to keep local copies of shared data; observation -perf of stateful servers can be extremely high, provided clients are allowed to keep local copies, as it turns out, reliability is often not a major problem;<br /><br />
[object server: click to view](./img/object-server.png), [click for example :ice runtime system](./img/server-and-client-example-ice-runtime-system.png),[[2]](./img/server-and-client-example-ice-runtime-system-2.png), [click for example :apache web server](./img/server-and-client-example-apache-web-server.png);<br /><br />
[server clusters: (i) click for 3 different tiers](./img/server-clusters-three-different-tiers.png);<br /><br />
(ii) crucial element: first tier is generally responsible for passing req to an appropriate server -req dispatching;<br /><br />
(iii) req handling: observation -having first tier handle all comm from/to the cluster may lead to a bottleneck, [solution -tcp handoff, click to view](./img/request-handling-solution-tcp-handoff.png);<br /><br />
(iv) when servers are spread across the internet: observation -spreading servers across the internet may introduce administrative problems, these can be largely circumvented by using data centers from a single cloud provider; req dispatching :if locality is important -common approach :use DNS, now, client looks up specific service through dns -client's ip addr is part of req, dns server keeps track of replica servers for the requested service and returns addr of most local server; client transparency -to keep client unaware of distribution, let dns resolver act on behalf of client, problem lies in the resolver may actually be far from local to the actual client;<br /><br />
(v) [click for a simplified version of the akamai cdn](./img/simplified-version-of-akamai-cdn.png), note: the cache is often sophisticated enough to hold more than just passive data, much of the application code of the origin server can be moved to the cache as well;<br /><br />
(vi) reasons to migrate code: load distribution -ensuring that servers in a data center are sufficiently loaded such as to prevent waste of energy, minimizing comm by ensuring that computations are close to where the data is(think of mobile computing); [click for flexibility :moving code to a client when needed](./img/migrate-code-reason-flexibility.png), here, it avoids pre-installing sw and increases dynamic config; privacy and security -in many cases, one cannot move data to anothe location, for whatever reason(often legal ones), solution is to move the code to the data, example :[federated ml, click to view](./img/federated-machine-learning.png);<br /><br />
(vii) [click for paradigms for code mobility](./img/paradigms-for-code-mobility.png);<br /><br />
(viii) strong and weak mobility: object components including: code segment -contains the actual code, data segment -contains the state, exec state -contains context of thread executing the object's code; weak mobility -move only code and data segment(and reboot exec), now, relatively simple especially if code is portable, distinguish code shipping(push) from code fetching(pull); strong mobility -move component including exec state, now, migration -move entire object from one machine to the other, cloning -start a clone and set it in the same exec state;<br /><br />
(ix) migration in heterogeneous systems: main problem -target machine may not be suitable to execute the migrated code, definition of process/thread/processor context is highly dependent on local hw, os, and runtime system; only solution -abstract machine implemented on different platforms, now, interpreted langs effectively having their own vm, vm monitors; observation -as containers are directly dependent on the underlying os, their migration in heterogeneous environments is far from trivial, to simply impractical, just as process migration is; migrating a vm -migrating imgs through 3 alternatives: pushing mem pages to the new machine and resending the ones that are later modified during the migration process, stopping the current vm then migrate mem and start the new vm, letting the new vm pull in new pages as needed, i.e., processes start on the new vm immediately and copy mem pages on demand; perf of migrating a vm -problem lies in a complete migration may actually take tens of sec, we also need to realize that during the migration, a service will be completely unavailable for multiple sec, [click for measurements regarding repsonse times during vm migration](./img/measurements-regarding-response-times-during-vm-migration.png).<br /><br />
线程和操作系统：主要问题 - 操作系统内核是否应该提供线程，还是应该将其实现为用户级包？用户空间解决方案 - 所有操作都可以在单个进程内完全处理，因此实现可以非常高效，内核提供的所有服务都是代表线程所在进程完成的，因此如果内核决定阻止某个线程，那么整个进程也将被阻止，当有许多外部事件时使用线程 - 线程根据每个事件进行阻止，因此如果内核无法区分线程，它如何支持向它们发送事件信号？；内核解决方案 - 整个想法是让内核包含线程包的实现，这意味着所有操作都作为系统调用返回，即当内核在同一进程内调度另一个可用线程时，阻止线程的操作不再是问题，当内核（捕获所有事件）调度与事件关联的线程时，处理外部事件很简单，问题是（或曾经是）效率损失，因为每个线程操作都需要一个内核陷阱；结论 -但是，尝试将用户级和内核线程混合成一个概念，然而，性能提升并没有被证明通常能够抵消增加的复杂性，这里，[结合用户级和内核线程的基本思想是引入一种两级线程方法-可以执行用户级线程的内核线程，点击查看](./img/combining-userlevel-and-kernel-threads.png)，这里，用户级和内核线程结合的原理操作包括：用户级线程执行系统调用，因此，执行该用户线程的内核线程被阻塞，用户级线程仍然绑定到内核线程；内核可以调度另一个绑定有可运行用户线程的内核线程，注意：这个用户级线程可以切换到当前在用户空间中的任何其他可运行的用户级线程；用户级线程调用阻塞的用户级操作，因此，进行上下文切换到可运行的用户级线程（然后绑定到同一个内核线程）；当没有用户级线程需要调度时，内核线程可能保持空闲状态，甚至可能被内核移除（销毁）；<br /><br />
在客户端使用线程：(i) 多线程 Web 客户端：隐藏网络延迟 -Web 浏览器扫描传入的 html 页面，发现需要获取更多文件；每个文件由单独的线程获取，每个线程执行（阻塞）http 请求；当文件进入时，浏览器显示它们；<br /><br />
(ii) 对其他机器的多个请求响应调用（rpc）：客户端同时进行多个调用，每个调用由不同的线程执行；然后等待，直到所有结果都返回；注意：如果调用不同的服务器，我们可能会线性加速；<br /><br />
(iii) 多线程客户端 - 有帮助吗？ TLP -线程级并行性：让 *ci* 表示恰好 *i* 个线程同时执行的时间分数，*TLP=sigma_{i=1}{N}i*ci / 1-c0*，这里，*N* 是可以同时执行的最大线程数；实际测量 - 典型的 Web 浏览器的 TLP 值介于 1.5-2.5 之间，因此线程主要用于逻辑组织浏览器；<br /><br />
在服务器端使用线程：(i) 提高性能：启动线程比启动新进程更便宜；单线程服务器禁止简单地扩展到多处理器系统；与客户端一样 - 通过在上一个请求正在回复时对下一个请求做出反应来隐藏网络延迟；<br /><br />
(ii) 更好的结构：大多数服务器都有很高的 io 需求，使用简单易懂的阻塞调用可以简化结构；由于控制流简化，多线程程序往往更小、更易于理解；<br /><br />
为什么多线程如此流行 -org: [dispatcher/worker 模型，点击查看](./img/dispatcher-worker-model.png);概述（模型和特征） -多线程 -并行性、阻塞系统调用、单线程进程 -无并行性、阻塞系统调用、有限状态机 -并行性、非阻塞系统调用；<br /><br />
虚拟化：（i）观察 -虚拟化很重要，因为：硬件变化比软件快，易于移植和代码迁移，隔离故障或受攻击的组件；<br /><br />
（ii）原理 -[模仿接口，点击查看](./img/virtualization-mimicking-interfaces.png)，这里有 3 个不同级别的 4 种接口：指令集架构 -机器指令集，有 2 个子集：特权指令 -仅允许由操作系统执行，通用指令 -可以由任何程序执行；操作系统提供的系统调用； lib 调用，称为 api；<br /><br />
(iii) [虚拟化方式，点击查看](./img/ways-of-virtualization.png)，这里的区别在于：进程 vm 是一组独立的指令，一个解释器/模拟器，运行在操作系统之上；本机 vmm 是低级指令，以及最基本的最小操作系统；托管 vmm 是低级指令，但委托了大部分工作到成熟的操作系统；<br /><br />
(iv) 放大虚拟机：性能：[完善组织，单击查看](./img/vm-performance-refining-organizations.png)；特殊指令包括：控制敏感指令 - 可能影响机器的配置，例如影响重定位寄存器或中断表的配置，行为敏感指令 - 效果部分由上下文决定，例如 *POPF* 设置中断启用标志，但仅限于系统模式；<br /><br />
(v) 虚拟化条件：必要条件 - 对于任何传统计算机，如果该计算机的敏感指令集是特权指令集的子集，则可以构造虚拟机监视器；问题：条件并不总是满足 - 可能存在在用户模式下执行的敏感指令，而不会导致操作系统陷入陷阱；解决方案 - 模拟所有指令，包装非特权敏感指令以将控制权转移到 vmm，半虚拟化 - 通过阻止非特权敏感指令或使其不敏感（即更改上下文）来修改客户操作系统；<br /><br />
(vi) [容器，单击查看](./img/containers.png)：命名空间 - 容器中的集合或进程被赋予自己的标识符视图，联合文件系统 - 将多个文件系统组合成分层方式，只有最高层允许 *写* 操作（并且是容器的一部分），控制组 - 可以对进程集合施加资源限制；示例：[planetlab，单击查看]（./img/containers-example-planetlab.png）：本质 - 不同的组织贡献机器，随后他们共享这些机器进行各种实验，问题 - 我们需要确保不同的分布式应用程序不会互相干扰，因此，虚拟化，Vserver - 独立且受保护的环境具有自己的库、服务器版本等，分布式应用程序被分配了分布在多台机器上的虚拟服务器集合；planetlab 虚拟服务器和切片：本质 - 每个虚拟服务器都在自己的环境中运行。 *chroot*，linux 增强功能包括适当调整进程 ID，例如 *init* 具有 ID0，不同 vserver 中的 2 个进程可能具有相同的用户 ID，但并不意味着是同一个用户，[分离导致切片 -单击查看](./img/vserver-separation-leads-to-slices.png)；<br /><br />
(vii) vms 和云计算：3 种类型的云服务包括：涵盖基础基础设施的 IaaS、涵盖系统级服务的 PaaS、包含实际应用程序的 SaaS，这里，IaaS - 云提供商不会出租物理机器，而是出租可能与其他客户共享物理机器的 vm（或 vmm），因此几乎完全与客户隔离（尽管可能无法达到性能隔离）； [客户端-服务器交互，点击区分应用级和中间件级解决方案](./img/distinguish-application-level-and-middleware-level-solutions.png)，例如：[X Window 系统，点击查看](./img/x-window-system.png)，这里有 X 客户端和服务器 - 应用程序充当 X 内核的客户端，后者作为客户端机器上的服务器运行，此外，通过以下方式改进 X：实际观察 - 应用程序逻辑和用户界面命令之间通常没有明确的分离，应用程序倾向于以与 X 内核紧密同步的方式运行，替代方法包括：让应用程序完全控制显示到像素级别（例如 VNC），仅提供一些高级显示操作（依赖于本地视频驱动程序）以允许更高效的显示操作；<br /><br />
(viii) 虚拟桌面环境：逻辑开发 - 随着基于云的应用程序越来越多，问题是如何从用户的角度使用这些应用程序前提？问题 - 开发终极网络用户界面，答案 - 使用 Web 浏览器建立无缝体验，[单击查看 Web 浏览器的结构](./img/anatomy-of-a-web-browser.png);<br /><br />
客户端软件：通常针对分发透明度量身定制：访问透明度 - 用于 rpcs 的客户端存根；位置/迁移透明度 - 让客户端软件跟踪实际位置；[复制透明度 - 由客户端存根处理的多个调用，单击查看](./img/client-side-software-replication-transparency.png);故障透明度 - 通常只能放在客户端（我们试图掩盖服务器和通信故障）；<br /><br />
服务器：（i）一般组织：基本模型 - 代表一组客户端实现特定服务的进程，它等待来自客户端的传入请求，随后确保请求得到处理，之后等待下一个传入请求；2 种基本类型：迭代服务器 - 服务器在处理下一个请求之前处理请求，并发服务器 - 使用调度程序，调度程序接收传入的请求，然后将其传递到单独的线程/进程；观察 - 并发服务器是常态，它们可以轻松处理多个请求，特别是在存在阻塞操作（到磁盘或其他服务器）的情况下；<br /><br />
（ii）联系人设置服务器：观察 -[大多数服务都绑定到特定端口，单击查看](./img/contacting-a-server.png)；[单击可通过 2 种方法动态分配端点](./img/dynamically-assigning-an-endpoint-through-two-approaches.png)；<br /><br />
(iii) 带外通信：问题 - 服务器接受（或正在接受）服务请求后，是否可以中断服务器？解决方案 1 - 使用单独的端口处理紧急数据，现在，服务器有单独的线程/进程处理紧急消息，紧急消息进入，因此相关请求被搁置，注意：我们要求操作系统支持基于优先级的调度；解决方案 2 - 使用传输层的功能，例如：tcp 允许在同一个连接中发送紧急消息，现在，可以使用操作系统信令技术捕获紧急消息；<br /><br />
(iv) 服务器和状态：无状态服务器 - 在处理完请求后，永远不会保留有关客户端状态的准确信息，现在，不记录文件是否已打开（访问后只需再次关闭它），不承诺使客户端的缓存无效，不跟踪客户端；后果 - 客户端和服务器完全独立，由于客户端和服务器崩溃而导致的状态不一致减少，可能造成性能损失，因为服务器无法预测客户端行为（考虑获取文件块）；问题 - 面向连接的通信是否适合无状态设计？；有状态服务器 - 跟踪其客户端的状态，现在，记录文件已打开，以便可以进行预取，知道客户端缓存了哪些数据，并允许客户端保留共享数据的本地副本；观察 - 有状态服务器的性能可能非常高，只要允许客户端保留本地副本，事实证明，可靠性通常不是一个主要问题；<br /><br />
[对象服务器：单击查看](./img/object-server.png)，[单击例如：ice 运行时系统](./img/server-and-client-example-ice-runtime-system.png)，[[2]](./img/server-and-client-example-ice-runtime-system-2.png)，[单击例如：apache web 服务器](./img/server-and-client-example-apache-web-server.png)；<br /><br />
[服务器集群：(i) 单击查看 3 个不同层级](./img/server-clusters-three-different-tiers.png)；<br /><br />
(ii) 关键要素：第一层通常负责将请求传递给适当的服务器 - 请求调度；<br /><br />
(iii) 请求处理：观察 - 让第一层处理来自/到集群的所有通信可能会导致瓶颈，[解决方案 - tcp 切换，单击查看](./img/request-handling-solution-tcp-handoff.png)；<br /><br />
(iv) 当服务器分布在互联网上时：观察 - 将服务器分布在互联网上可能会引入管理问题，这些问题可以通过使用来自单个云提供商的数据中心来很大程度上规避；请求调度：如果位置很重要 - 常见方法：使用 DNS，现在，客户端通过 dns 查找特定服务 - 客户端的 ip 地址是请求的一部分，dns 服务器跟踪请求服务的副本服务器并返回大多数本地服务器的地址；客户端透明度 - 让客户端不知道分布情况，让 DNS 解析器代表客户端行事，问题在于解析器实际上可能远离实际客户端的本地；<br /><br />
(v) [单击查看简化版 akamai cdn](./img/simplified-version-of-akamai-cdn.png)，注意：缓存通常足够复杂，可以保存不仅仅是被动数据，原始服务器的许多应用程序代码也可以移动到缓存中；<br /><br />
(vi) 迁移代码的原因：负载分布 - 确保数据中心的服务器负载充足，以防止能源浪费，通过确保计算靠近数据所在位置来最大限度地减少通信（想想移动计算）；[单击以获得灵活性：在需要时将代码移动到客户端](./img/migrate-code-reason-flexibility.png)，在这里，它避免了预安装软件并增加了动态配置；隐私和安全 - 在许多情况下，无论出于何种原因（通常是合法原因），都无法将数据移动到另一个位置，解决方案是将代码移动到数据，例如：[联合机器学习，单击查看](./img/federated-machine-learning.png);<br /><br />
(vii) [单击查看代码移动性的范例](./img/paradigms-for-code-mobility.png);<br /><br />
(viii) 强移动性和弱移动性：对象组件包括：代码段 - 包含实际代码，数据段 - 包含状态，执行状态 - 包含执行对象代码的线程的上下文；弱移动性 - 仅移动代码和数据段（并重新启动执行），现在，相对简单，特别是如果代码是可移植的，区分代码传送（推送）和代码获取（拉取）；强移动性 - 移动组件（包括执行状态）、现在、迁移 - 将整个对象从一台机器移动到另一台机器、克隆 - 启动克隆并将其设置为相同的执行状态；<br /><br />
(ix) 异构系统中的迁移：主要问题 - 目标机器可能不适合执行迁移的代码、进程/线程/处理器上下文的定义高度依赖于本地硬件、操作系统和运行时系统；唯一的解决方案 - 在不同平台上实现的抽象机器，现在，解释语言实际上拥有自己的虚拟机、虚拟机监视器；观察 - 由于容器直接依赖于底层操作系统，它们在异构环境中的迁移绝非易事，甚至不切实际，就像进程迁移一样；迁移虚拟机 - 通过 3 种替代方案迁移 imgs：将 mem 页面推送到新机器并重新发送在迁移过程中稍后修改的页面，停止当前虚拟机，然后迁移 mem 并启动新虚拟机，让新虚拟机根据需要拉入新页面，即，进程立即在新虚拟机上启动并按需复制 mem 页面；迁移虚拟机的性能 - 问题在于完整的迁移实际上可能需要几十秒，我们还需要意识到在迁移期间，服务将在几秒钟内完全不可用，[单击以查看有关虚拟机迁移期间响应时间的测量结果](./img/measurements-regarding-response-times-during-vm-migration.png)。<br /><br />
****
### Communication
[basic networking model, click to view](./img/basic-networking-model.png): drawbacks including: focus on msg-passing only, often unneeded or unwanted functionality, violates access transparency;<br /><br />
low level layers including: physical layer -contains the specification and implementation of bits, and their transmission bt sender and receiver, data link layer -prescribes transmission of a series of bits into a frame to allow for error and flow control, network layer -describes how packets in a network of computers are to be routed; observation -for many distributed systems, the lowest-level interface is that of the network layer;<br /><br />
transport layer: provides the actual comm facilities for most distributed systems; standard internet protocols including: tcp -connection-oriented, reliable, stream-oriented commu, udp -unreliable (best-effort) datagram comm;<br /><br />
mw layer: observation -mw is invented to provide common services and protocols that can be used by many different applications, it is (i)a rich set of comm protocols, (ii)(un)marshaling of data, necessary for integrated systems, (iii)naming protocols, to allow easy sharing of resources, (iv)security protocols for secure comm, (v)scaling mechanisms such as for replication and caching; note: what remains are truly application-specific protocols, [click for an adpated layering scheme](./img/an-adapted-layering-scheme.png);<br /><br />
[types of comm: click for distinguish](./img/distinguish-types-of-communication.png), here, transient vs. persistent comm -former is comm server discards msg when it cannot be delivered at the next server or at the receiver, latter is a msg is stored at a comm server as long as it takes to deliever it, asynchronous vs. synchronous comm :places for sync including: at req submission, at req delivery, after req processing;<br /><br />
client/server: observations -client/server computing is generally based on a model of transient, now, synchronous comm it is (i)client and server have to be active at the time of comm, (ii)client issues req and blocks until it receives reply, (iii)server essentially waits only for incoming req, and subsequently processes them, synchronous comm drawbacks including: client cannot do any other work while waiting for reply, failures have to be handled immediately :client is waiting, the model may simply not be appropriate such as mail, news;<br /><br />
msging: msg-oriented mw -aims at high-level persistent asynchronous comm, now, processes send each other msg which are queued, sender need not wait for immediate reply but can do other things, mw often ensures fault tolerance;<br /><br />
[rpc: (i) basic op, click to view](./img/rpc-basic-operation.png): observations -application developers are familiar with simple procudure model, well-engineered procedures operate in isolation(black box), there is no fundamental reason not to execute procudures on separate machine; conclusion -comm bt caller and callee can be hidden by using [procesure-call mechanism, click to view](./img/procedure-call-mechanism.png);<br /><br />
(ii) param passing: there is more than just wrapping params into a msg including: client and server machines may have different data representations(think of byte ordering), wrapping a param means transforming a value into a sequence of bytes, client and server have to agree on the same encoding of: how are basic data values represented(int, f, char), how are complex data values represented(array, union); conclusion -client and server need to properly interpret msgs, transforming them into machine-dependent representations; some assumptions -copy in/out semantics is while procedure is executed nothing can be assumed about param values, all data is to be operated on is passed by params, excludes passing references to (global) data; conclusion -full access transparency cannot be realized; a remote reference mechanism enhances access transparency, now, remote reference offers unified access to remote data, remote reference can be passed as param in rpcs, note: stubs can sometimes be used as such references;<br /><br />
(iii) [asynchronous rpcs: essence -try to get ride of the strict req-reply behavios, but let the client continue without waiting for an answer from the server, click to view](./img/asynchronous-rpcs.png);<br /><br />
(iv) [sending out multiple rpcs: essence -sending an rpc req to a group of servers, click to view](./img/sending-out-multiple-rpcs.png);<br /><br />
transient msging :[sockets, click for python code](./img/socket-python-code.png): [berkeley socket interface including, click to view](./img/berkeley-socket-interface.png)(op&description): socket -create a new comm endpoint, bind -attach a local addr to a socket, listen -tell os what the max #pending connection reqs should be, accept -block caller until a connection req arrives, send -send some data over the connection, receive -receive some data over the connection, close -release the connection;<br /><br />
making sockets easier to work with: observation -sockets are rather low level and programming mistakes are easily made, however, the way that they are used is often the same(such as in a client-server setting); alternative :ZeroMQ -provides a higher level of expression by pairing sockets, one for sending msg at process P, and a corresponding one at process Q for receiving msg, all comm is asynchronous; 3 patterns including: [req-reply, click to view](./img/socket-pattern-request-reply.png), [publish-subscribe, click to view](./img/socket-pattern-publish-subscribe.png), [pipeline, click to view](./img/socket-pattern-pipeline.png);<br /><br />
mpi :when lots of flexibility is needed: representative ops including(op&description): MPI_BSEND -append outgoing msg to a local send buffer, MPI_SEND -send a msg and wait until copied to local or remote buffer, MPI_SSEND -send a msg and wait until transmission starts, MPI_SENDRECV -send a msg and wait for reply, MPI_ISEND -pass reference to outgoing msg and continue, MPI_ISSEND -pass reference to outgoing msg and wait until receipt starts, MPI_RECV -receive a msg block if there is none, MPI_IRECV -check if there is an incomig msg but do not block;<br /><br />
queue-based msging: click for 4 possible combinations(./img/queue-based-messaging-four-combinations.png);<br /><br />
msg-oriented mw: essence -asynchronous persistent comm through support of mw-level queues, queues correspond to buffers at comm servers; ops including: PUT -append a msg to a specified queue, GET -block until thespecified queue is nonempty and remove first msg, POLL -check a specified queue for msgs and remove first, never block, NOTIFY -install a handler to be called when a msg is put into the specified queue;<br /><br />
general model: queue managers -queues are managed by queue managers, an application can put msgs only into a local queue, getting a msg is possible by extracting it from a local queue only, hence, queue managers need to route msgs; [click for routing](./img/routing.png);<br /><br />
[msg broker, click for general architecture](./img/message-broker-general-architecture.png): observation -meg queuing systems assume a common msging protocol -all applications agree on msg format(i.e., structure and data representation); broker handlers appllication heterogeneity in an MQ system, it is (i)transforms incoming msgs to target format, (ii)very often acts as an application gtway, (iii)may provide subject-based routing capabilities(i.e., publish-subscribe capabilities); example :[AMQP, click to view](./img/amqp-producer.png),[[2]](./img/amqp-consumer.png): [lack of standardization -advanced msg-queuing protocol was intended to play the same role as for example tcp in networks, i.e., a protocol for high-level msging with different implementations, click to view](./img/amqp-lack-of-standardization.png), basic model -client sets up a (stable) connection, which is a container for several (possibly ephemeral) one-way channels, 2 one-way channels can form a session, a link is akin to a socket, and maintains state about msg transfers;<br /><br />
application-level multicasting: essence -organize nodes of a distributed system into an overlay network and use that network to dissemniate data, it is (i)oftentimes a tree, leading to unique paths, (ii)alternatively also mesh networks, requiring a form of routing; application-level multicasting in chord -basic approach including: initiator generates a multicast identifier *mid*, lookup *succ(mid)* the node responsible for *mid*, req is routed to *succ(mid)* which will become the *root*, if *P* wants to join it sends a join req to the root, when req arrives at *Q*, case1, *Q* has not seen a join req before then it becomes forwarder, *P* becomes child of *Q*, join req continues to be forwarded, case2, *Q* knows about tree then *P* becomes child of *Q*, no need to forward join req anymore;<br /><br />
[ALM :some costs: click for different metrics](./img/alm-different-metrics.png) including: link stress -how often does an alm msg cross the same physical link? such as msg from A to D needs to cross <Ra,Rb> twice, stretch -ratio in delay bt alm-level path and network-level path, such as msgs from B to C follow path of length 73 at alm, but 47 at network level, hence, stretch=73/47;<br /><br />
[flooding: essence -P simply sends a msg m to each of its neighbors, each neighbor will forward that msg, except to P, and only if it had not seen m before, click to view](./img/flooding-essence.png); variation -let Q forward a msg with a certain prob *p_flood*, possibly even dependent on its own #neighbors(i.e., node degree) or the degree of its neighbors;<br /><br />
epidemic protocols: assume there are no write-write conflicts, now, update ops are performed at a single server, a replica passes updated state to only a few neighbors, update prop is lazy(i.e., not immediate), eventually each update should reach every replica; 2 forms of epidemics of: anti-entropy -each replica regularly chooses another replica at random and exchanges state diffs leading to identical states at both afterwards, rumor-spreading -a replica which has just been updated(i.e., has been contaminated) tells several other replicas about its update(contaminating as well);<br /><br />
anti-entropy: principle ops including: a node P selects another node Q from the system at random, PULL -P only pulls in new updates from Q, PUSH -P only pushes its own updates to Q, PUSH-PULL -P and Q send updates to each other; observation -for *push-pull* it takes *O(log(N))* rounds to disseminate updates to all *N* nodes, here, round=when every node has taken the initiative to start an exchange; analysis :basics -consider a single source, propagating its update, let *pi* be the prob that a node has not received the update after *i-th* round, analysis :staying ignorant -with *pull*, *p_i+1=(pi)^2*, the node was not updated during *i-th* round and should contact another ignorant node during the next round, with *push*, *p_i+1=pi\*(1-1/(N-1))^((N-1)\*(1-pi))=pie^-1(for small pi and large N)*, the node was ignorant during *i-th* round and no updated node chooses to contact it during the next round, with *push-pull*, *(pi)^2\*(pi\*e^-1)*; [perf, click to view](./img/anti-entropy-performance.png);<br /><br />
rumor spreading: basic model -a server S having an update to report, contacts other servers, if a server is contacted to which the update has already propagated, S stops contacting other servers with prob *pstop*; observation -if *s* is the fraction of ignorant servers(i.e., which are unaware of the update), it can be shown that with many servers: *s=e^-(1/pstop+1)(1-s)*; formal analysis -let *s* denote fraction of nodes that have not yet been updated(i.e., susceptible), *i* denote fraction of updated(infected) and active nodes, *r* denote fraction of updated nodes that gave up(removed), from theory of epidemics, *(1)ds/dt=-s\*i (2)di/dt=s\*i-pstop\*(1-s)\*i hence di/ds=-(1+pstop)+pstop/s hence i(s)=-(1+pstop)\*s+pstop\*ln(s)+C*, wrap up, *i(1)=0 hence C=1+pstop hence i(s)=(1+pstop)\*(1-s)+pstop\*ln(s)*, we are looking for the case *i(s)=0 hence s=e^-(1/pstop+1)(1-s)*; [click for effect of stoping](./img/rumor-spreading-effect-of-stopping.png), note: if we really have to ensure that all servers are eventually updated, rumor spreading alone is not enough;<br /><br />
deleting values: fundamental problem -we cannot remove an old value from a server and expect the removal to propagate, instead, mere removal will be undone in due time using epidemic algorithms, solution -removal has to be registered as a special update by inserting a death certificate, when to remove a death certificate(it is not allowed to stay forever) including: run a global algorithm to detect whether the removal is known everywhere and then collect the death certificates(looks like garbage collection), assume death certificates propagate in finite time and associate a max lifetime for a certificate(can be done at risk of not reaching all servers), note: it is necessary that a removal actually reaches all servers.<br /><br />
传输层：为大多数分布式系统提供实际的通信设施；标准互联网协议包括：tcp - 面向连接、可靠、面向流的通信，udp - 不可靠（尽力而为）数据报通信；<br /><br />
mw 层：观察 -mw 被发明用于提供可供许多不同应用程序使用的通用服务和协议，它是 (i) 一组丰富的通信协议，(ii)(反) 数据编组，集成系统所必需的，(iii) 命名协议，允许轻松共享资源，(iv) 用于安全通信的安全协议，(v) 扩展机制，例如用于复制和缓存；注意：剩下的是真正特定于应用程序的协议，[单击以查看适应的分层方案](./img/an-adapted-layering-scheme.png)；<br /><br />
[通信类型：单击以区分](./img/distinguish-types-of-communication.png)，这里，瞬态与持久通信 - 前者是通信服务器在无法在下一个服务器或接收器处传递消息时丢弃消息，后者是消息存储在通信服务器上，直到传递它为止，异步与同步通信：同步的位置包括：在请求提交时，在请求交付时，在请求处理后；<br /><br />
客户端/服务器：观察 - 客户端/服务器计算通常基于瞬态模型，现在，同步通信是 (i) 客户端和服务器在通信时必须处于活动状态，(ii) 客户端发出请求并阻塞直到收到回复， (iii)服务器本质上只是等待传入的请求，然后处理它们，同步通信的缺点包括：客户端在等待回复时不能做任何其他工作，必须立即处理故障：客户端正在等待，该模型可能根本不合适，例如邮件、新闻；<br /><br />
msging：面向消息的mw-旨在实现高级持久异步通信，现在，进程相互发送排队的消息，发送者不需要等待立即回复，而是可以做其他事情，mw通常确保容错；<br /><br />
[rpc：(i)基本操作，点击查看](./img/rpc-basic-operation.png)：观察-应用程序开发人员熟悉简单的程序模型，精心设计的程序在隔离（黑盒）中运行，没有根本原因不在单独的机器上执行程序；结论 - 通过使用 [过程调用机制，点击查看](./img/procedure-call-mechanism.png)，可以隐藏通信调用者和被调用者；<br /><br />
(ii) 参数传递：不仅仅是将参数包装成消息，还包括：客户端和服务器机器可能具有不同的数据表示（想想字节顺序），包装参数意味着将值转换为字节序列，客户端和服务器必须就以下编码达成一致：基本数据值如何表示（int、f、char），复杂数据值如何表示（数组、联合）；结论 - 客户端和服务器需要正确解释消息，将它们转换为与机器相关的表示；一些假设 - 复制输入/输出语义是在执行过程时不能假设参数值，所有要操作的数据都通过参数传递，不包括传递对（全局）数据的引用；结论 - 无法实现完全访问透明性；远程引用机制增强了访问透明度，现在，远程引用提供了对远程数据的统一访问，远程引用可以在 rpcs 中作为参数传递，注意：存根有时可以用作此类引用；<br /><br />
(iii) [异步 rpcs：本质 - 尝试摆脱严格的请求-回复行为，但让客户端继续而不等待服务器的答复，单击查看](./img/asynchronous-rpcs.png);<br /><br />
(iv) [发送多个 rpcs：本质 - 向一组服务器发送 rpc 请求，单击查看](./img/sending-out-multiple-rpcs.png);<br /><br />
transient msging :[sockets，单击查看 python 代码](./img/socket-python-code.png)：[berkeley socket 接口包括，单击查看view](./img/berkeley-socket-interface.png)(op&description): socket -创建一个新的通信端点，bind -将本地地址连接到套接字，listen -告诉操作系统最大#pending 连接请求应该是多少，accept -阻止调用者直到连接请求到达，send -通过连接发送一些数据，receive -通过连接接收一些数据，close -释放连接；<br /><br />
使套接字更易于使用：观察 -套接字级别较低，编程容易出错，但是，它们的使用方式通常是相同的（例如在客户端-服务器设置中）；替代方案：ZeroMQ -通过配对套接字提供更高级别的表达，一个用于在进程 P 发送消息，另一个用于在进程 Q 接收消息，所有通信都是异步的； 3种模式包括：[req-reply，点击查看](./img/socket-pattern-request-reply.png)、[publish-subscribe，点击查看](./img/socket-pattern-publish-subscribe.png)、[pipeline，点击查看](./img/socket-pattern-pipeline.png);<br /><br />
mpi :当需要大量灵活性时：代表性操作包括（操作和说明）：MPI_BSEND - 将传出消息附加到本地发送缓冲区，MPI_SEND - 发送消息并等待复制到本地或远程缓冲区，MPI_SSEND - 发送消息并等待传输开始，MPI_SENDRECV - 发送消息并等待回复，MPI_ISEND - 传递对传出消息的引用并继续，MPI_ISSEND - 传递对传出消息的引用并等待接收开始，MPI_RECV - 如果没有则接收消息块，MPI_IRECV - 检查是否有传入消息但不阻止；<br /><br />
基于队列的消息：单击可查看 4 种可能的组合(./img/queue-based-messaging-four-combinations.png)；<br /><br />
面向消息的mw：本质-通过支持mw级队列实现异步持久通信，队列对应于通信服务器上的缓冲区；操作包括：PUT-将消息附加到指定队列，GET-阻塞直到指定队列非空并删除第一个消息，POLL-检查指定队列中的消息并删除第一个消息，永不阻塞，NOTIFY-安装一个处理程序，当将消息放入指定队列时调用；<br /><br />
一般模型：队列管理器-队列由队列管理器管理，应用程序只能将消息放入本地队列，只能通过从本地队列中提取消息才能获取消息，因此，队列管理器需要路由消息； [单击以查看路由](./img/routing.png);<br /><br />
[消息代理，单击以查看一般架构](./img/message-broker-general-architecture.png)：观察 -meg 排队系统假设一个通用的消息协议 -所有应用程序都同意消息格式（即结构和数据表示）；代理处理程序在 MQ 系统中的应用程序异构性，它 (i) 将传入的消息转换为目标格式，(ii) 经常充当应用程序网关，(iii) 可以提供基于主题的路由功能（即发布-订阅功能）；示例：[AMQP，单击查看](./img/amqp-producer.png)，[[2]](./img/amqp-consumer.png)：[缺乏标准化 - 高级消息队列协议旨在发挥与网络中的 tcp 相同的作用，即具有不同实现的高级消息协议，单击查看](./img/amqp-lack-of-standardization.png)，基本模型 - 客户端建立一个（稳定的）连接，它是多个（可能是短暂的）单向通道的容器，2 个单向通道可以形成一个会话，一个链接类似于一个套接字，并维护有关消息传输的状态；<br /><br />
应用程序级多播：本质 - 将分布式系统的节点组织成一个覆盖网络并使用该网络传播数据，它（i）通常是一棵树，导致唯一路径，(ii) 或者也可以是网状网络，需要某种形式的路由；chord 中的应用级多播 - 基本方法包括：发起者生成多播标识符 *mid*，查找负责 *mid* 的节点 *succ(mid)*，请求被路由到将成为 *root* 的 *succ(mid)*，如果 *P* 想要加入，它会向根发送加入请求，当请求到达 *Q* 时，case1，*Q* 之前没有见过加入请求，然后它成为转发者，*P* 成为 *Q* 的子节点，加入请求继续被转发，case2，*Q* 知道树，然后 *P* 成为 *Q* 的子节点，不再需要转发加入请求；<br /><br />
[ALM：一些成本：单击以获取不同的指标](./img/alm-different-metrics.png) 包括：链路压力 - alm 消息穿过同一物理链路的频率是多少？例如从 A 到 D 的消息需要经过 <Ra,Rb> 两次，延时比为 alm 级路径和网络级路径的延时长，例如从 B 到 C 的消息在 alm 处遵循长度为 73 的路径，但在网络级路径上遵循长度为 47 的路径，因此，延时比为 73/47；<br /><br />
[flooding：essence -P 只是向其每个邻居发送一条消息 m，每个邻居都会转发该消息，但 P 除外，并且仅当它之前没有见过 m 时，才转发，单击查看](./img/flooding-essence.png)；变化 - 让 Q 以一定的概率 *p_flood* 转发消息，甚至可能取决于其自己的邻居数（即节点度）或其邻居的度；<br /><br />
流行病协议：假设没有写入冲突，现在，更新操作在单个服务器上执行，副本仅将更新状态传递给少数邻居，更新 prop 是惰性的（即不是即时的），最终每个更新都应该到达每个副本；两种形式的流行病：反熵 - 每个副本定期随机选择另一个副本并交换状态差异，导致事后两个副本的状态相同；谣言传播 - 刚刚更新的副本（即已被污染）将其更新告知其他几个副本（同样被污染）；<br /><br />
反熵：主要操作包括：节点 P 从系统中随机选择另一个节点 Q；PULL -P 仅从 Q 中提取新更新；PUSH -P 仅将自己的更新推送到 Q；PUSH-PULL -P 和 Q 相互发送更新；观察 - 对于 *push-pull*，需要 *O(log(N))* 轮才能将更新传播到所有 *N* 个节点，这里，轮数=当每个节点都主动开始交换；分析：基础-考虑一个单一来源，传播其更新，让*pi*表示节点在第*i*轮之后未收到更新的概率，分析：保持无知-使用*pull*，*p_i+1=(pi)^2*，该节点在第*i*轮期间未更新，应在下一轮中联系另一个无知节点，使用*push*，*p_i+1=pi\*(1-1/(N-1))^((N-1)\*(1-pi))=pie^-1(对于较小的 pi 和较大的 N)*，该节点在第*i*轮期间是无知的，并且在下一轮中没有更新的节点选择联系它，使用*push-pull*，*(pi)^2\*(pi\*e^-1)*； [perf，点击查看](./img/anti-entropy-performance.png);<br /><br />
谣言传播：基本模型 - 服务器 S 有更新要报告，联系其他服务器，如果联系的服务器已经传播了更新，S 会停止联系其他服务器，概率为 *pstop*；观察 - 如果 *s* 是无知服务器（即不知道更新的服务器）的比例，则可以证明，对于许多服务器：*s=e^-(1/pstop+1)(1-s)*；正式分析 - 让 *s* 表示尚未更新（即易受感染）的节点比例，*i* 表示已更新（感染）和活跃节点比例，*r* 表示放弃（删除）的已更新节点比例，根据流行病理论，*(1)ds/dt=-s\*i (2)di/dt=s\*i-pstop\*(1-s)\*i 因此 di/ds=-(1+pstop)+pstop/s 因此 i(s)=-(1+pstop)\*s+pstop\*ln(s)+C*，总结，*i(1)=0 因此 C=1+pstop 因此 i(s)=(1+pstop)\*(1-s)+pstop\*ln(s)*，我们正在寻找情况 *i(s)=0 因此 s=e^-(1/pstop+1)(1-s)*； [点击查看停止的效果](./img/rumor-spreading-effect-of-stopping.png)，注意：如果我们真的要确保所有服务器最终都得到更新，单靠谣言传播是不够的；<br /><br />
删除值：根本问题 - 我们不能从服务器中删除旧值并期望删除会传播，相反，单纯的删除将使用流行算法在适当的时候撤消，解决方案 - 删除必须通过插入死亡证明注册为特殊更新，何时删除死亡证明（不允许永远保留）包括：运行全局算法来检测是否到处都知道删除，然后收集死亡证明（看起来像垃圾收集），假设死亡证明在有限的时间内传播并为证书关联最大生存期（可以冒着无法到达所有服务器的风险来完成），注意：删除必须真正到达所有服务器。<br /><br />
****
### Coordination
physical clocks: problem -sometimes we simply need the exact time not just an ordering, solution :UTC -universal coordinated time, it is (i)based on #transistors per sec of the cesium 133 atom(pretty accurate), (ii)at present the real time is taken as the average of some 50 cesium clocks around the world, (iii)introduces a leap sec from time to time to compensate that days are getting older, note: utc is broadcast through short-wave radio and satellite, satellites can give an accuracy of about +-0.5ms;<br /><br />
clock sync: precision -goal is to keep the deviation bt 2 clocks on any 2 machines within a specified bound, known as the precision $\pi$: *for any t,p,q: |Cp(t)-Cq(t)|<=*$\pi$ with $Cp(t)$ the computed clock time of machine *p* at utc time *t*; accuracy -in the case of accuracy we aim to keep the clock bound to a value $\alpha$: *for any t,p: |Cp(t)-t|<=*$\alpha$; sync of: internal sync -keep clocks precise, external sync -keep clocks accurate;<br /><br />
clock drift: clock specifications including: a clock comes specified with its max clock drift rate $\rho$, *F(t)* denotes oscillator freq of the hw clock at time *t*, *F* is the clock's ideal (constant) freq hence living up to specifications: *for any t: (1-\rho)<=F(t)/F<=(1+\rho)*; observation -by using hw interrupts we couple a sw clock to the hw clock, and thus also its clock drift rate: *Cp(t)=1/F\*f_{0}{t}F(t)dt hence dCp(t)/dt=F(t)/F hence for any t: 1-\rho<=dCp(t)/dt<=1+\rho*; [click for fast, perfect, slow clocks](./img/fast-perfect-slow-clocks.png);<br /><br />
detecting and adjusting incorrect times: [click for getting current time from a timeserver](./img/getting-current-time-from-a-timeserver.png); computing the relative offset $\theta$ and delay $\delta$: *assumption: \delta Treq=T2-T1=T4-T3=\delta Tres, hence \theta =T3+((T2-T1)+(T4-T3))/2-T4=((T2-T1)+(T3-T4))/2, \delta =((T4-T1)+(T3-T2))/2*; network time protocol -collect($\theta$, $\delta$) pairs, choose $\theta$ for which associated delat $\delta$ was minimal;<br /><br />
reference broadcast synchornization: essence -a node broadcasts a reference msg *m* hence each receiving node *p* records the time *T_p,m* that it received *m*, note: *T_p,m* is read from *p*'s local clock; problem -averaging will not capture drift hence use linear regression: *NO: Offset\[p,q\](t)=sigma_{k=1}{M}(T_p,k-T_q,k)/M YES: Offset\[p,q\](t)=\alpha t+\beta*; [RBS minimizes critical path, click to view](./img/rbs-minimizes-critical-path.png);<br /><br />
the happened-before relationship: issue -what usually matters is not that all processes agree on exactly what time it is, but that they agree on the order in which events occur, requires a notion of ordering; the happened-before relation: if a and b are 2 events in the same process, and a comes before b, then a->b, if a is the sending of a msg, and b is the receipt of that msg, then a->b, if a->b and b->c, then a->c; note: this introduces a partial ordering of events in a system with concurrently operating processes;<br /><br />
logical clocks: problem -how do we maintain a global view of the system's behavior that is consistent with the happened-before relation? attach a timestamp C(e) to each event e, satisfying the following properties: P1 if a and b are 2 events in the same process, and a->b, then we demand that C(a)<C(b), P2 if a corresponds to sending a msg m, and b to the receipt of that msg, then also C(a)<C(b); problem -how to attach a timestamp to an event when there is no global block, hence, maintain a consistent set of logical clocks, 1 per process? each process Pi maintains a local counter Ci and adjusts this counter, in the following steps: for each new event that takes place Pi, Ci is incremented by 1, each time a msg m is sent by process Pi, the msg receives a timestamp ts(m)=Ci, whenever a msg m is received by a process Pj, Pj adjusts its local counter Cj to max{Cj,ts(m)}, then executes step1 before passing m to the application; note: property P1 is satisfied by step1, property P2 by step2&3, it can still occur that 2 events happen at the same time, avoid this by breaking ties through process IDs; [example: click for considering 3 processes with event counters operating at different rates](./img/logical-clocks-example.png); [where implemented: click for adjustments implemented in mw](./img/logical-clocks-where-implemented.png);<br /><br />
[example :totally ordered multicast: concurrent updates on a replicated db are seen in the same order everywhere, click to view](./img/totally-ordered-multicast.png), it is (i)P1 adds $100 to an account(initial value $1000), (ii)P2 increments account by 1%, (iii)there are 2 replicas; results: in absence of proper synchronization: replica #1 <- $1111, while replica #2 <- $1110; solution: process Pi sends timestamped msg mi to all others, the msg itself is put in a local queue queuei, any incoming msg at Pj is queued in queuej, according to its timestamp, and acknowledged to every other process; Pj passes a msg mi to its application if: mi is at the head of queuej, for each process Pk, there is a msg mk in queuej with a larger timestamp; note: we are assuming that comm is reliable and fifo ordered; [click for lamport's clocks for mutual exclusion](./img/lamports-clocks-for-mutual-exclusion.png),[[2]](./img/lamports-clocks-for-mutual-exclusion-2.png), here, it is analogy with ordered multicast -with totally ordered multicast, all processes build identical queues, delivering msgs in the same order, mutual exclusion is about agreeing in which order processes are allowed to enter a critical region;<br /><br />
vector clocks: observation -lamport's clocks do not guarantee that if C(a)<C(b) that a causally preceded b, [click for concurrent msg transmission using logical clocks](./img/concurrent-message-transmission-using-logical-clocks.png), note: we cannot conclude that a causally precedes b; causal dependency -defined as we say that b may causally depend on a if ts(a)<ts(b) with: for all k, ts(a)\[k\]<=ts(b)\[k\] and there exists at least 1 index k' for which ts(a)\[k'\]<ts(b)\[k'\], precedence vs. dependency is we say that a casually precedes b, b may causally depend on a as there may be info from a that is propagated into b; capturing potential causality -solution :each Pi maintains a vector VCi, here, VCi is the local logical clock at process Pi, and if VCi\[j\]=k then Pi knows that k events have occured at Pj, maintaining vector clocks by: before executing an event Pi executes VCi\[i\]<-VCi\[i\]+1, when process Pi sends a msg m to Pj it sets m's (vector) timestamp ts(m) equal to VCi after having executed former step, upon receipt of a msg m process Pj sets VCj\[k\]<-max{VCj\[k\],ts(m)\[k\]} for each k after which it executes first step and then delievers the msg to the application; [example :capturing potential causality when exchanging msgs, click to view](./img/vector-clocks-example.png); causally ordered multicasting -observation that we can now ensure that a msg is delievered only if all causally preceding msgs have already been delievered, adjustment that Pi increments VCi\[i\] only when sending a msg and Pj adjusts VCj when receiving a msg(i.e., effectively does not change VCj\[j\]), Pj postpones delivery of m until: *ts(m)\[i\]=VCj\[i\]+1 ts(m)\[k\]<=VCj\[k\] for all k not equal to i*, [click for enforcing causal comm](./img/enforcing-causal-communication-in-casually-ordered-multicasting.png), here, example: take VC3=\[0,2,2\], ts(m)=\[1,3,0\] from P1, what info does P3 have, and what will it do when receiving m(from P1)?<br /><br />
mutual exclusion: problem -several processes in a distributed system want exclusive access to some resource, basic solution -permission-based that a process wanting to enter its critical region or access a resource needs permission from other processes, token-based that a token is passed bt processes the one who has the token may proceed in its critical region or pass it on when not interested; [click for permission-based](./img/mutual-exclusion-permission-based.png), centralized, simply use a coordinator, it is (i)process P1 asks the coordinator for permission to access a shared resource, permission is granted, (ii)process P2 then asks permission to access the same resource, the coordinator does not reply, (iii)when P1 releases the resource, it tells the coordinator, which then replies to P2; ricart&agrawala -the same as lamport except that acknowledgments are not sent, i.e., return a response to a req only when: the receiving process has no interest in the shared resource, or, the receiving process is waiting for the resource but has lower priority(known through comparison of timestamps), in all cases reply is deferred, implying some more local administration, [click for example with 3 processes](./img/mutual-exclusion-ricart-and-agrawala-example.png), here, 2 processes want to access a shared resource at the same moment, P0 has the lowest timestamp so it wins, when process P0 is done it sends an OK also so P2 can now go ahead; token ring algorithm -essence that organize processes in a logical ring and let a token be passed bt them, the one that holds the token is allowed to enter the critical region(if it wants to), [click for an overlay network constructed as a logical ring with a circulating token](./img/); decentralized mutual exclusion -principle that assume every resource is replicated N times with each replica having its own coordinator, hence, access requires a majority vote from m>N/2 coordinators, here, a coordinator always responds immediately to a req, assumption that when a coordinator crashes it will recover quickly but will have forgotten about permissions it had granted, [click for how robust is this system?](./img/decentralized-mutual-exclusion-how-robust-the-system-is.png), [click for violation prob for various param values](./img/decentralized-mutual-exclusion-violation-probabilities-for-various-parameter-values.png); conclusion: [click to view](./img/mutual-exclusion-conclusion.png), [click for example :zookeeper](./img/mutual-exclusion-example-zookeeper.png),[[2]](./img/mutual-exclusion-example-zookeeper-2.png),[[3]](./img/mutual-exclusion-example-zookeeper-3.png),[[4]](./img/mutual-exclusion-example-zookeeper-4.png);<br /><br />
election algorithms: principle -an algorithm requires that some process acts as a coordinator, the question is how to select this special process dynamically, note -in many systems the coordinator is chosen manually such as file servers, this leads to centralized solutions, hence, single point of failure, teasers -if a coordinator is chosen dynamically to what extent can we speak about a centralized or distributed solution? if a fully distributed solution i.e. one without a coordinator, always more robust than any centralized/coordinated solution? basic assumptions -all processes have unique IDs, all processes kwow IDs of all processes in the system(but not if they are up or down), election means identifying the process with the highest ID that is up; [election by bullying, click to view](./img/election-algorithm-by-bullying.png) -principle that consider N processes {P0,...,P_N-1} and let id(Pk)=k when a process Pk notices that the coordinator is no longer responding to reqs it initiates an election in the following steps: Pk sends an ELECTION msg to all processes with higher identifiers P_k+1,...,P_N-1, if no one responds Pk wins the election and becomes coordinator, if one of the higher-ups answers it takes over and Pk's job is done; [election in a ring, click to view](./img/election-algorithm-using-a-ring.png)(the solid line shows the election msgs initiated by P6, the dashed one denotes the msgs by P3) -principle that process priority is obtained by organizing processes into a (logical) ring, the process with the highest priority should be elected as coordinator, it is (i)any process can start an election by sending an election msg to its successor, if a successor is down, the msg is passed on to the next successor, (ii)if a msg is passed on the sender adds itself to the list, when it gets back to the initiator, everyone had a chance to make its presence known, (iii)the initiator sends a coordinator msg around the ring containing a list of all living processes, the one with the highest priority is elected as coordinator, [click for example :leader election in zookeeper server group](./img/leader-election-in-zookeeper-server-group.png),[[2]](./img/leader-election-in-zookeeper-server-group-2.png), [click for example :leader election in raft](./img/leader-election-in-raft.png),[[2]](./img/leader-election-in-raft-2.png); [election by proof of work, click to view](./img/election-by-proof-of-work.png),[[2]](./img/election-by-proof-of-work-2.png); [election by proof of stake, click to view](./img/election-by-proof-of-stake.png); [click for a solution for wireless networks](./img/a-solution-for-wireless-networks.png),[[2]](./img/a-solution-for-wireless-networks-2.png),[[3]](./img/a-solution-for-wireless-networks-3.png);<br /><br />
gossip-based coordination :aggregation: typical applications including: data dissemination -perhaps the most import one, note: there are many variants of dissemination, aggregation -let every node Pi maintain a variable vi, when 2 nodes gossip, they each reset their variable to *vi,vj<-(vi+vj)/2*, result: in the end each node will have computed the average *v^bar=sigma_i vi/N*, what happens in the case that initially *vi=1* and *vj=0,j not equal to i*?;<br /><br />
gossip-based coordination :peer sampling: problem -for many gossip-based applications, you need to select a peer uniformly at random from the entire network, in principle, this means you need to know all other peers, impossible? basics including: each node maintains a list of *c* references to other nodes, regularly pick another node at random(from the list) and exchange roughly *c/2* references, when the application needs to select a node at random it also picks a random one from its local list; observation -statistically it turns out that selection of a peer from the local list is indistinguishable from selecting uniformly at random peer from the entire network;<br /><br />
[gossip-based overlay construction, click to view(./img/gossip-based-overlay-construction.png)]: essence -maintain 2 local lists of neighbors, the lowest list is used for providing a peer-sampling service, the highest list is used to carefully select application-dependent neighbors; [click for a 2d torus](./img/gossip-based-overlay-construction-a-2d-torus.png),[[2]](./img/gossip-based-overlay-construction-a-2d-torus-python.png);<br /><br />
secure gossiping: [dramatic attack -consider when exchanging references, a set of colluding nodes systematically returns links only to each other, hence, we are dealing with hub attack, click to view](./img/secure-gossiping-dramatic-attack.png), here, situation that a network with 100000 nodes, a local list size *c*=30, and only 30 attackers, the y-axis shows #nodes with links only to the attackers, after less than 300 rounds, attackers have full control; [a solution :gathering stats -this is what measuring indegree distributions tells us, that, which fraction of nodes(y-axis) have how many other nodes pointing to them(x-axis)? click to view](./img/secure-gossiping-dramatic-attack-solution.png), basic approach that when a benign node initiates an exchange it may either use the result for gathering stats, or, for updating its local list, an attacker is in limbo, that, will its response be used for statistical purposes or for functional purposes? observation that when gathering stats may reveal colluders a colluding node will be forced to behave according to the protocol;<br /><br />
[distributed event matching, clicl to view](./img/distributed-event-matching.png): principle -a process specifies in which events it is interested(subscription *S*), and when a process publishes a notification *N* we need to see whether *S* matches *N*; hard part lies in implementing the match function in a scalable manner;<br /><br />
[click for general approach](./img/centralized-implementations-general-approach.png);<br /><br />
[click for selective routing](./img/selective-routing.png);<br /><br />
[click for gossiping :sub2sub](./img/gossiping-sub2sub.png),[[2]](./img/gossiping-sub2sub-2.png);<br /><br />
secure publish-subscribe: we are facing nasty dilemmas including: referential decoupling that msgs should be able to flow from a publisher to subscribers while guaranteeing mutual anonymity, hence, we cannot set up a secure channel, not knowing where msgs come from imposes integrity problems, assuming a trusted broker may easily be practically impossible certainly when dealing with sensitive info, hence, we now have a routing problem; solution -allow for searching(and matching) on encrypted data without the need for decryption through [PEKS -public-key encryption with keyword search, click for its basics](./img/public-key-encryption-with-keyword-search.png);<br /><br />
GPS -global positioning system: (i) positioning nodes: issue -in large-scale distributed systems in which nodes are dispersed across a wide-area network, we often need to take some notion of proximity or distance into account, hence, it starts with determining a (relative) location of a node;<br /><br />
(ii) computing position: observation -a node P needs d+1 landmarks to compute its own position in a d-dim space, [consider 2-dim case, click to view](./img/computing-position-in-2d.png);<br /><br />
(iii) gps: assuming that clocks of satellites are accurate and synchronized, i.e., it takes a while before a signal reaches receiver, and receiver's clock is definitely out of sync with satellite, [click for basics](./img/gps-basics.png);<br /><br />
wifi-based location services: basic idea including: assume we have a db of known APs -access points with coordinates, assume we can estimate distance to an AP, then: with 3 detected APs, we can compute a position; war driving :locating APs, it is (i)use a wifi-enabled device along with a gps receiver and move through an area while recording observed APs, compute the centroid by assuming an AP has been detected at N different locations {x1^->,...,xN^->} with known gps location, compute location of AP as xAP^->=(sigma_{i=1}{N}xi^->)/N; problems including: limited accuracy of each gps detection point xi^->, an AP has a nonuniform transmission range, #sampled detection points N may be too low; [click for computing position](./img/wifi-based-computing-position.png),[[2]](./img/wifi-based-computing-position-2.png),[[3]](./img/wifi-based-computing-position-3.png).<br /><br />
物理时钟：问题 - 有时我们只需要精确的时间而不仅仅是一个顺序，解决方案：UTC - 世界协调时间，它 (i) 基于铯 133 原子每秒的晶体管数量（相当准确），(ii) 目前真实时间取为全球约 50 个铯钟的平均值，(iii) 不时引入闰秒以补偿日子越来越老，注意：utc 通过短波无线电和卫星广播，卫星可以提供大约 +-0.5ms 的精度;<br /><br />
时钟同步：精度 - 目标是将任何两台机器上的 2 个时钟之间的偏差保持在指定的范围内，称为精度 $\pi$：*对于任何 t、p、q：|Cp(t)-Cq(t)|<=*$\pi$，其中 $Cp(t)$ 是机器 *p* 在 utc 时间 *t* 计算出的时钟时间；精度 - 在精度的情况下，我们的目标是将时钟绑定到一个值 $\alpha$：*对于任何 t,p：|Cp(t)-t|<=*$\alpha$；同步：内部同步 - 保持时钟精确，外部同步 - 保持时钟准确；<br /><br />
时钟漂移：时钟规格包括：时钟指定其最大时钟漂移率 $\rho$，*F(t)* 表示时间 *t* 时硬件时钟的振荡器频率，*F* 是时钟的理想（恒定）频率，因此符合规格：*对于任何 t：(1-\rho)<=F(t)/F<=(1+\rho)*；观察 - 通过使用硬件中断，我们将软件时钟与硬件时钟耦合，从而将其时钟漂移率耦合：*Cp(t)=1/F\*f_{0}{t}F(t)dt 因此 dCp(t)/dt=F(t)/F 因此对于任何 t：1-\rho<=dCp(t)/dt<=1+\rho*；[点击获取快速、完美、慢速时钟](./img/fast-perfect-slow-clocks.png)；<br /><br />
检测和调整不正确的时间：[点击从时间服务器获取当前时间](./img/getting-current-time-from-a-timeserver.png)；计算相对偏移 $\theta$ 和延迟 $\delta$：*假设：\delta Treq=T2-T1=T4-T3=\delta Tres，因此 \theta =T3+((T2-T1)+(T4-T3))/2-T4=((T2-T1)+(T3-T4))/2，\delta =((T4-T1)+(T3-T2))/2*；网络时间协议 - 收集（$\theta$，$\delta$）对，选择相关延迟 $\delta$ 最小的 $\theta$；<br /><br />
参考广播同步：本质 - 一个节点广播一个参考消息 *m*，因此每个接收节点 *p* 记录它接收 *m* 的时间 *T_p,m*，注意：*T_p,m* 是从 *p* 的本地时钟读取的；问题 - 平均不会捕获漂移，因此使用线性回归：*否：Offset\[p,q\](t)=sigma_{k=1}{M}(T_p,k-T_q,k)/M 是：Offset\[p,q\](t)=\alpha t+\beta*; [RBS 最小化关键路径，单击查看](./img/rbs-minimizes-critical-path.png);<br /><br />
发生于之前的关系：问题 - 通常重要的不是所有进程都同意确切的时间，而是它们同意事件发生的顺序，这需要排序的概念；发生于之前的关系：如果 a 和 b 是同一进程中的两个事件，并且 a 在 b 之前发生，则 a->b，如果 a 是发送消息，而 b 是接收该消息，则 a->b，如果 a->b 且 b->c，则 a->c；注意：这在具有并发操作进程的系统中引入了事件的部分排序；<br /><br />
逻辑时钟：问题 - 我们如何维护与发生前关系一致的系统行为的全局视图？将时间戳 C(e) 附加到每个事件 e，满足以下属性：P1 如果 a 和 b 是同一进程中的 2 个事件，并且 a->b，则我们要求 C(a)<C(b)，P2 如果 a 对应于发送消息 m，而 b 对应于接收该消息，则 C(a)<C(b)；问题 - 如何在没有全局块的情况下将时间戳附加到事件，从而维护一组一致的逻辑时钟，每个进程 1 个？每个进程 Pi 维护一个本地计数器 Ci，并按照以下步骤调整该计数器：对于每个发生的新事件 Pi，Ci 都会加 1；每当进程 Pi 发送一个消息 m 时，该消息都会收到一个时间戳 ts(m)=Ci；每当进程 Pj 收到一个消息 m 时，Pj 都会调整其本地计数器 Cj 为 max{Cj,ts(m)}，然后执行步骤 1 后再将 m 传递给应用程序；注意：步骤 1 满足属性 P1，步骤 2 和 3 满足属性 P2；仍然可能发生 2 个事件同时发生的情况，可通过进程 ID 打破平局来避免这种情况；[示例：单击以考虑 3 个事件计数器以不同速率运行的进程](./img/logical-clocks-example.png); [实施位置：单击查看 mw 中实施的调整](./img/logical-clocks-where-implemented.png);<br /><br />
[示例：完全有序多播：复制数据库上的并发更新在任何地方都以相同的顺序显示，单击查看](./img/totally-ordered-multicast.png)，它是 (i)P1 向帐户添加 100 美元（初始值 1000 美元），(ii)P2 将帐户增加 1%，(iii) 有 2 个副本；结果：在没有正确同步的情况下：副本 #1 <- $1111，而副本 #2 <- $1110；解决方案：进程 Pi 将带时间戳的消息 mi 发送给所有其他进程，消息本身被放入本地队列queuei，Pj 处的任何传入消息都根据其时间戳在queuej 中排队，并向其他所有进程确认；如果 mi 位于queuej 的头部，对于每个进程Pk，queuej 中都有一个具有较大时间戳的消息mk，则 Pj 将消息 mi 发送给其应用程序；注意：我们假设通信是可靠的并且按先进先出顺序排列； [点击查看 Lamport 的互斥时钟](./img/lampors-clocks-for-mutual-exclusion.png),[[2]](./img/lampors-clocks-for-mutual-exclusion-2.png), 这里，它类似于有序多播 - 对于完全有序多播，所有进程都建立相同的队列，以相同的顺序传递消息，互斥是关于同意进程以何种顺序进入临界区;<br /><br />
向量时钟：观察 - Lamport 的时钟不能保证如果 C(a)<C(b)，则 a 因果先于 b，[点击查看使用逻辑时钟的并发消息传输](./img/concurrent-message-transmission-using-logical-clocks.png), 注意：我们不能得出 a 因果先于 b 的结论；因果依赖 - 定义为如果 ts(a)<ts(b)，则我们说 b 可能因果依赖于 a：对于所有 k，ts(a)\[k\]<=ts(b)\[k\] 并且存在至少 1 个索引 k' 使得 ts(a)\[k'\]<ts(b)\[k'\]，优先与依赖是我们说 a 偶然先于 b，b 可能因果依赖于 a，因为可能有来自 a 的信息传播到 b 中；捕获潜在因果关系 -解决方案：每个 Pi 维护一个向量 VCi，这里，VCi 是进程 Pi 的本地逻辑时钟，如果 VCi\[j\]=k 则 Pi 知道在 Pj 发生了 k 个事件，通过以下方式维护向量时钟：在执行事件之前，Pi 执行 VCi\[i\]<-VCi\[i\]+1，当进程 Pi 向 Pj 发送消息 m 时，它在执行完前面的步骤后将 m 的（向量）时间戳 ts(m) 设置为 VCi，收到消息 m 后，进程 Pj 为每个 k 设置 VCj\[k\]<-max{VCj\[k\],ts(m)\[k\]}，然后执行第一步，再将消息发送给应用程序；[示例：在交换消息时捕获潜在因果关系，单击查看](./img/vector-clocks-example.png);因果有序多播 - 观察我们现在可以确保只有在所有因果前置消息都已传送的情况下，消息才会被传送，调整为 Pi 仅在发送消息时增加 VCi\[i\]，而 Pj 在接收消息时调整 VCj（即实际上不会改变 VCj\[j\]），Pj 推迟传送 m 直到：对于所有不等于 i 的 k，*ts(m)\[i\]=VCj\[i\]+1 ts(m)\[k\]<=VCj\[k\]*，[单击以强制因果通信](./img/enforcing-causal-communication-in-casually-ordered-multicasting.png)，此处，示例：从 P1 中取 VC3=\[0,2,2\]、ts(m)=\[1,3,0\]， P3 有什么信息，当从 P1 接收到 m 时它会做什么？<br /><br />
互斥：问题 - 分布式系统中的多个进程想要独占访问某些资源，基本解决方案 - 基于权限，即想要进入其关键区域或访问资源的进程需要其他进程的权限，基于令牌，即向进程传递令牌，拥有令牌的进程可以在其关键区域中继续前进，或者在不感兴趣时​​将其传递下去；[单击以查看基于权限的](./img/mutual-exclusion-permission-based.png)，集中式，只需使用协调器，它是 (i) 进程 P1 向协调器请求访问共享资源的权限，权限被授予，(ii) 进程 P2 然后请求访问相同资源的权限，协调器不回复，(iii) 当 P1 释放资源时，它会告诉协调器，然后协调器回复 P2； ricart&agrawala -与 lampor 相同，只是不发送确认，即，仅在以下情况下返回对请求的响应：接收进程对共享资源不感兴趣，或者，接收进程正在等待资源但优先级较低（通过比较时间戳得知），在所有情况下，回复都会被推迟，意味着更多的本地管理，[单击例如 3 个进程](./img/mutual-exclusion-ricart-and-agrawala-example.png)，这里，2 个进程想要同时访问共享资源，P0 具有最低的时间戳所以它获胜，当进程 P0 完成时它也会发送 OK，因此 P2 现在可以继续；令牌环算法 - 本质是将进程组织成一个逻辑环并让令牌在它们之间传递，持有令牌的进程被允许进入关键区域（如果它愿意的话），[单击构建为具有循环令牌的逻辑环的覆盖网络](./img/);分散式互斥 - 假设每个资源被复制 N 次，每个副本都有自己的协调器，因此，访问需要来自 m>N/2 个协调器的多数票，这里，协调器总是立即响应请求，假设当协调器崩溃时它会很快恢复，但会忘记它已授予的权限，[单击查看该系统的稳健性如何？](./img/decentralized-mutual-exclusion-how-robust-the-system-is.png)，[单击查看各种参数值的违规概率](./img/decentralized-mutual-exclusion-violation-probabilities-for-various-parameter-values.png)；结论：[单击查看](./img/mutual-exclusion-conclusion.png), [例如单击：zookeeper](./img/mutual-exclusion-example-zookeeper.png),[[2]](./img/mutual-exclusion-example-zookeeper-2.png),[[3]](./img/mutual-exclusion-example-zookeeper-3.png),[[4]](./img/mutual-exclusion-example-zookeeper-4.png);<br /><br />
选举算法：原则 - 算法要求某个进程充当协调器，问题是如何动态地选择这个特殊进程，注意 - 在许多系统中，协调器是手动选择的，例如文件服务器，这会导致集中式解决方案，因此，单点故障，预告片 - 如果协调器是动态选择的，那么我们可以在多大程度上谈论集中式或分布式解决方案？如果完全分布式解决方案，即没有协调员的解决方案，是否总是比任何集中式/协调式解决方案更强大？基本假设 - 所有进程都有唯一的 ID，所有进程都知道系统中所有进程的 ID（但不知道它们是启动还是关闭），选举意味着识别 ID 最高的启动进程；[通过欺凌进行选举，点击查看](./img/election-algorithm-by-bullying.png) - 原则考虑 N 个进程 {P0,...,P_N-1} 并让 id(Pk)=k 当进程 Pk 注意到协调员不再响应请求时，它会按照以下步骤启动选举：Pk 向所有具有更高标识符 P_k+1,...,P_N-1 的进程发送 ELECTION 消息，如果没有人响应，Pk 赢得选举并成为协调员，如果其中一个更高级别的进程回答，它就会接管，Pk 的工作就完成了； [环中的选举，点击查看](./img/election-algorithm-using-a-ring.png)(实线表示由 P6 发起的选举消息，虚线表示由 P3 发起的消息) - 通过将进程组织成（逻辑）环来获得进程优先级的原理，优先级最高的进程应被选为协调器，它是（i）任何进程都可以通过向其后继者发送选举消息来启动选举，如果后继者关闭，则该消息将传递给下一个后继者，（ii）如果消息被传递，发送者会将自己添加到列表中，当它返回到发起者时，每个人都有机会表明自己的存在，（iii）发起者向环发送一个协调器消息，其中包含所有活动进程的列表，优先级最高的进程被选为协调器，[例如单击：zookeeper 服务器中的领导者选举组](./img/leader-election-in-zookeeper-server-group.png),[[2]](./img/leader-election-in-zookeeper-server-group-2.png), [点击例如：raft 中的 leader 选举](./img/leader-election-in-raft.png),[[2]](./img/leader-election-in-raft-2.png); [通过工作量证明进行选举，点击查看](./img/election-by-proof-of-work.png),[[2]](./img/election-by-proof-of-work-2.png); [通过权益证明进行选举，点击查看](./img/election-by-proof-of-stake.png); [点击获取无线网络解决方案](./img/a-solution-for-wireless-networks.png),[[2]](./img/a-solution-for-wireless-networks-2.png),[[3]](./img/a-solution-for-wireless-networks-3.png);<br /><br />
基于八卦的协调：聚合：典型应用包括：数据传播 - 也许是最重要的一个，注意：传播、聚合有很多变体 - 让每个节点 Pi 维护一个变量 vi，当 2 个节点八卦时，它们各自重置其变量为 *vi,vj<-(vi+vj)/2*，结果：最终每个节点都会计算出平均值 *v^bar=sigma_i vi/N*，如果最初 *vi=1* 且 *vj=0,j 不等于 i*，会发生什么情况？;<br /><br />
基于八卦的协调:对等点抽样：问题 - 对于许多基于八卦的应用程序来说，您需要从整个网络中均匀随机地选择一个对等点，原则上，这意味着您需要了解所有其他对等点，这不可能吗？基本原理包括：每个节点维护一个对其他节点的 *c* 个引用列表，定期随机选择另一个节点（从列表中）并交换大约 *c/2* 个引用，当应用程序需要随机选择一个节点时，它还会从其本地列表中随机选择一个节点；观察 - 从统计上看，从本地列表中选择一个对等点与从整个网络中均匀随机选择对等点没有区别；<br /><br />
[基于八卦的覆盖构造，单击查看(./img/gossip-based-overlay-construction.png)]：本质 - 维护 2 个邻居本地列表，最低列表用于提供对等点抽样服务，最高列表用于仔细选择依赖于应用程序的邻居； [点击查看 2d 圆环](./img/gossip-based-overlay-construction-a-2d-torus.png),[[2]](./img/gossip-based-overlay-construction-a-2d-torus-python.png);<br /><br />
安全八卦：[戏剧性攻击 - 考虑在交换引用时，一组串通节点系统地只向彼此返回链接，因此，我们正在处理中心攻击，点击查看](./img/secure-gossiping-dramatic-attack.png)，这里的情况是，一个有 100000 个节点的网络，本地列表大小 *c*=30，并且只有30 个攻击者，y 轴显示仅与攻击者有链接的节点数，经过不到 300 轮，攻击者就完全控制了；[解决方案：收集统计数据 - 这是测量入度分布告诉我们的，即哪一部分节点（y 轴）有多少其他节点指向它们（x 轴）？点击查看](./img/secure-gossiping-dramatic-attack-solution.png)，基本方法是当良性节点发起交换时，它可以使用结果来收集统计数据，或者用于更新其本地列表，攻击者处于不确定状态，其响应将用于统计目的还是功能目的？观察发现，在收集统计数据时可能会发现合谋者，合谋节点将被迫按照协议行事；<br /><br />
[分布式事件匹配，点击查看](./img/distributed-event-matching.png)：原理 - 一个进程指定它感兴趣的事件（订阅 *S*），当一个进程发布通知 *N* 时，我们需要查看 *S* 是否与 *N* 匹配；困难的部分在于以可扩展的方式实现匹配功能；<br /><br />
[点击查看一般方法](./img/centralized-implementations-general-approach.png);<br /><br />
[点击查看选择性路由](./img/selective-routing.png);<br /><br />
[点击查看 gossiping :sub2sub](./img/gossiping-sub2sub.png),[[2]](./img/gossiping-sub2sub-2.png);<br /><br />
安全发布-订阅：我们面临着棘手的困境，包括：引用解耦，消息应该能够从发布者流向订阅者，同时保证相互匿名，因此，我们无法建立安全通道，不知道消息来自哪里会带来完整性问题，假设一个值得信赖的经纪人在处理敏感信息时可能很容易变得不可能，因此，我们现在有一个路由问题；解决方案 - 允许搜索（和匹配）加密数据，而无需通过 [PEKS - 带关键字搜索的公钥加密，单击查看其基础知识](./img/public-key-encryption-with-keyword-search.png) 解密；<br /><br />
GPS - 全球定位系统：(i) 定位节点：问题 - 在节点分散在广域网中的大规模分布式系统中，我们经常需要考虑一些接近度或距离的概念，因此，它从确定节点的（相对）位置开始；<br /><br />
(ii) 计算位置：观察 - 节点 P 需要 d+1 个地标来计算其在 d 维空间中的位置，[考虑 2 维情况，单击查看](./img/computing-position-in-2d.png)；<br /><br />
(iii) gps：假设卫星时钟准确且同步，即需要一段时间在信号到达接收器之前，接收器的时钟肯定与卫星不同步，[点击查看基础知识](./img/gps-basics.png);<br /><br />
基于 wifi 的位置服务：基本思想包括：假设我们有一个已知 AP 的数据库 - 带坐标的接入点，假设我们可以估计到 AP 的距离，然后：通过 3 个检测到的 AP，我们可以计算出一个位置；战争驾驶：定位 AP，它是 (i) 使用支持 wifi 的设备以及 gps 接收器并在记录观察到的 AP 的同时穿过一个区域，通过假设在 N 个不同的位置 {x1^->,...,xN^->} 检测到 AP 来计算质心，已知 gps 位置，计算 AP 的位置为 xAP^->=(sigma_{i=1}{N}xi^->)/N；问题包括：每个 gps 检测点 xi^-> 的精度有限，AP 具有不均匀的传输范围，#sampled 检测点 N 可能太低； [点击计算位置](./img/wifi-based-computing-position.png),[[2]](./img/wifi-based-computing-position-2.png),[[3]](./img/wifi-based-computing-position-3.png)。
****
### Naming
names are used to denote entities in a distributed system, to operate on an entity, we need to access it at an access point, access points are entities that are named by means of an addr, note：a location-independent name for an entity E, is independent of the addrs of the access points offered by E;<br /><br />
identifiers: pure name -a name that has no meaning at all, it is just a random string, pure names can be used for comparison only, identifier -a name having some specific properties including: an identifier refers to at most one entity, each entity is referred to by at most one identifier, an identifier always refers to the same entity(i.e., it is never reused), observation -an identifier need not necessarily be a pure name(i.e., it may have content);<br /><br />
broadcasting: broadcast the ID, requesting the entity to return its current addr -can never scale beyond lan, requires all processes to listen to incoming location requests; ARP -addr resolution protocol -to find out which mac addr is associated with an ip addr, broadcast the query "who has this ip addr"?<br /><br />
forwarding pointers: when an entity moves, it leaves behind a pointer to its next location -dereferencing can be made entirely transparent to clients by simply following the chain of pointers, update a client's reference when present location is found, geographical scalability problems(for which separate chain reduction mechanisms are needed) including: long chains are not fault tolerant, increased network latency at dereferencing;<br /><br />
home-based approaches: single-tiered scheme -let a home keep track of where the entity is, it is (i)entity's home addr registered at a naming service, (ii)the home registers the foreign addr of the entity, (iii)client contacts the home first and then continues with foreign location; [click for principle of mobile IP](./img/principle-of-mobile-ip.png); problems including: home addr has to be supported for entity's lifetime, home addr is fixed, hence unnecessary burden when the entity permanently moves, poor geographical scalability(entity may be next to client), note: permanent moves may be tackled with another level of naming -dns;<br /><br />
illustrative :chord: consider the org of many nodes into a logical ring, it is (i)each node is assigned a random m-bit identifier, (ii)every entity is assigned a unique m-bit key, (iii)entity with key k falls under jurisdiction of node with smallest id>=k -called its successor succ(k); nonsolution: let each node keep track of its neighbor and start linear search along the ring; notation: speak of node p as the node have identifier p;<br /><br />
chord finger tables: principle -each node *p* maintains a finger table *FTp[]* with at most m entries: *FTp\[i\]=succ(p+2^(i-1))*, note: the *i*-th entry points to the first node succeeding p by at least *2^(i-1)*; to look up a key *k*, node *p* forwards the request to node with index *j* satisfying: *q=FTp\[j\]<=k<FTp\[j+1\]*; if *p<k<FTp\[1\]*, the request is also forwarded to *FTp\[1\]*; [click for chord lookup example](./img/chord-lookup-example.png), [click for chord in python](./img/chord-in-python.png);<br /><br />
exploit network proximity: problem -the logical org of nodes in the overlay may lead to erratic msg transfers in the underlying internet -node p and node succ(p+1) may be very far apart; solutions including: topology-aware node assignment -when assigning an ID to a node, make sure that nodes close in the ID space are also close in the network, can be very difficult, proximity routing -maintain more than 1 possible succ, and forward to the closest, example: in chord *FTp\[i\]* points to first node in *INT=\[p+2^(i-1), p+2^i-1\]*, node p can also store pointers to other nodes in *INT*, proximity neighbor selection -when there is a choice of selecting who your neighbor will be(not in chord), pick the closest one;<br /><br />
HLS -hierarchical location services: (i) basic idea: build a large-scale search tree for which the underlying network is divided into hierarchical domains, each domain is represented by a separate directory node, [click for principle](./img/hierarchical-location-services-principle.png);<br /><br />
(ii) tree org: invariants including: addr of entity E is stored in a leaf or intermediate node, intermediate nodes contain a pointer to a child iff the subtree rooted at the child stores an addr of the entity, the root knows about all entities, [click for storing info of an entity having 2 addrs in different leaf domains](./img/storing-information-of-an-entity-having-two-addresses-in-different-leaf-domains.png);<br /><br />
(iii) lookup op: start lookup at local leaf node, node knows about E then follow downward pointer else go up, upward lookup always stops at root, [click for looking up a location](./img/looking-up-a-location.png);<br /><br />
(iv) insert op: [an insert op is forwarded to the first node that knows about entity E, a chain of forwarding pointers to the leaf node is created, click to view](./img/insert.png);<br /><br />
(v) can an hls scale? observation -a design flaw seems to be that the root node needs to keep track of all identifiers, hence make a distinction bt a logical design and its physical implementation; notation including: assume there are a total of *N* physical hosts *{H1,...HN}* each host is capable of running 1 or more location servers, *Dk(A)* denotes the domain at level *k* that contains addr *A* *k=0* denotes the root domain, *LSk(E,A)* denotes the unique location server in *Dk(A)* responsible for keeping track of entity *E*; basic idea for scaling -choose different physical servers for the logical name servers on a per-entity basis(at root level, but also intermediate), implement a mapping of entities to physical servers such that the load of storing records will be distributed, solution -*Dk={D_k,1, ..., D_k,Nk}* denotes the *Nk* domains at level *k*, note: *N0=|D0|=1*, for each level *k* the set of hosts is partitioned into *Nk* subsets with each host running a location server representing exactly one of the domains *D_k,i* from *Dk*, [click for principle of distributing logical location servers](./img/principle-of-distributing-logical-location-servers.png);<br /><br />
security in flat naming: basics -without special measures we need to trust that the name-resolution process to return what is associated with a flat name, 2 approaches to follow: secure the identifier2entity association, secure the name-resolution process; self-certifying names -use a value derived from the associated entity and make it(part of) the flat name: *id(name)=hash(data associated with the entity)* when dealing with read-only entities, *id(entity)=public key(entity)* otherwise, in which case additional data is returned such as a verifiable digital signature; securing the name-resolution process -much more involved until discussing secure dns;<br /><br />
name space: naming graph -a graph in which a leaf node represents a (named) entity, a dir node is an entity that refers to other nodes, [click for a general naming graph with a single root node](./img/a-general-naming-graph-with-a-single-root-node.png); we can easily store all kinds of attributes in a node including: type of the entity, an identifier for that entity, addr of the entity's location, nicknames, etc., note: dir nodes can also have attributes, besides just storing a dir table with (identifier,label) pairs;<br /><br />
name resolution: problem -to resolve a name we need a dir node, how do we actually find that (initial) node? closure mechanism -the mechanism to select the implicit context from which to start name resolution, examples including: www.distributed-systems.net -start at a dns name server, /home/maarten/mbox -start at the local nfs file server(possible recursive search), 0031 20 598 7784 -dial a phone number, 77.167.55.6 -route msg to a specific ip addr, note: you cannot have an explicit closure mechanism, how would you start?<br /><br />
name linking: hard link -what we have described so far as a path name :a name that is resolved by following a specific path in a naming graph from one node to another; soft link -allow a node N to contain a name of another node, it is (i)first resolve *N*'s name(leading to *N*), read the content of *N* yielding *name*, name resolution continues with *name*; observation -the name resolution process determines that we read the content of a node, in particular, the name in the other node that we need to go to, one way or the other, we know where and how to start name resolution given *name*; [click for concept of a symbolic link explained in a naming graph](./img/concept-of-a-symbolic-link-explained-in-a-naming-graph.png);<br /><br />
mounting: issue -name resolution can also be used to merge different name spaces transparently through mounting by associating a node identifier of another name space with a node in a current name space; terminology including: foreign name space -the name space that needs to be accessed, mount point -the node in the current name space containing the node identifier of the foreign name space, mounting point -the node in the foreign name space where to continue name resolution; mounting across a network including: the name of an access protocol, the name of the server, the name of the mounting point in the foreign name space; [click for mounting in distributed systems](./img/mounting-in-distributed-systems.png);<br /><br />
name space implementation: issue -distribute the name resolution as well as name space mgmt across multiple machines by distributing nodes of the naming graph; distinguish 3 levels including: global level -consists of the high-level dir nodes, main aspect is that these dir nodes have to be jointly managed by different administrations, administrational level -contains mid-level dir nodes that can be grouped in such a way that each group can be assigned to a separate administration, managerial level -consists of low-level nodes within a single administration, main issue is effectively mapping dir nodes to local name servers, [click for name space implementation](./img/name-space-implementation.png), [click for a comp bt name servers for implementing nodes in a name space](./img/a-comparison-between-name-servers-for-implementing-nodes-in-a-name-space.png), [click for iterative name resolution](./img/iterative-name-resolution.png), [click for recursive name resolution](./img/recursive-name-resolution.png), [click for caching in recursive name resolution](./img/caching-in-recursive-name-resolution.png); scalabiity issues including: size scalability -we need to ensure that servers can handle a large number of requests per time unit hence high-level servers are in big trouble, solution -assume (at least at global and administrative level) that content of nodes hardly ever changes, we can then apply extensive replication by mapping nodes to multiple servers, and start name resolution at the nearest server, observation -an important attributed of many nodes is the addr where the represented entity can be contacted, replicating nodes makes large-scale traditional name servers unsuitable for locating mobile entities, [click for need to ensure name resolution process scales across large geographical distances](./img/need-to-ensure-name-resolution-process-scales-across-large-geographical-distances.png);<br /><br />
dns: essence -hierarchically organized name space with each node having exactly 1 incoming edge hence edge label=node label, here, domain :a subtree, domain name :a path name to a domain's root node; info in a node including(type&refers2&description): SOA, zone, holds info on the represented zone, A, host, ip addr of host this node represents, MX, domain, mail server to handle mail for this node, SRV, domain, server handling a specific service, NS, zone, name server for the represented zone, CNAME, node, symbolic link, PTR, host, canonical name of a host, HINFO, host, info on this host, TXT, any kind, any info considered useful, [click for modern dns](./img/modern-dns.png), [click for secure dns](./img/secure-dns.png),[[2]](./img/secure-dns-2.png); [click for naming in nfs](./img/naming-in-nfs.png), [click for mounting nested dirs](./img/mounting-nested-directories.png), attribute-based naming -observation that in many cases it is much more convenient to name and lookup entities through their attributes hence [traditional dir services(aka yellow pages), click to view](./img/implementing-directory-services.png), here, [click for LDAP -lightweight dir access protocol](./img/lightweight-directory-access-protocol.png),[[2]](./img/lightweight-directory-access-protocol-2.png), problem that lookup ops can be expensive as they require matching requested attribute values against actual attribute values hence inspect all entities(in principle);<br /><br />
[distributed index: click to view](./img/distributed-index.png),[[2]](./img/distributed-index-2.png),[[3]](./img/distributed-index-3.png),[[4]](./img/distributed-index-4.png);<br /><br />
[NDN -named-data networking: click to view](./img/named-data-networking.png); basics -retrieve an entity from the network by using that entity's name and not addr, the network takes that name as input and routes a req to a location where the entity is stored, ndn takes over the role of ip in a future architecture of the internet; example name: /distributed-systems.net/books/Distributed Systems/4/01/Naming;<br /><br />
routing: question -is there really a difference attempting to route a req such as: distributed-systems.net/books/Distributed Systems/4/01/Naming from the IPv6 addr: 2001:610:508:192:87:108:15? key observation -there is no fundamental difference, we decide which part of a name or addr(i.e., a prefix) should be announced within a global routing substrate, just as with IPv4 addrs with bgp routers; [click to view](./img/routing.png).
****



